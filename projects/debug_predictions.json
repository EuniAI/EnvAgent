{
    "flink": {
        "project_name": "flink",
        "project_language": "Java",
        "project_repo_url": "https://github.com/apache/flink",
        "project_image": "flink_image",
        "success": true,
        "playground_path": "/tmp/tmpdij0kphk/flink",
        "container_info": null,
        "testsuite_states": {
            "testsuite_max_refined_query_loop": 4,
            "testsuite_context_provider_messages": [
                {
                    "type": "SystemMessage",
                    "content": "content=\"You are to discover ONE minimal, safe, and quick verification shell command from README/docs to confirm the environment/setup works (e.g., '<tool> --version', '<package> --help', 'make check', 'pytest -q'). Avoid destructive or long-running commands.\" additional_kwargs={} response_metadata={} id='c17dbd4b-6dd4-44ed-9251-59ee43b8b930'",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "HumanMessage",
                    "content": "content='Find one quick verification command from README/docs for this repository.' additional_kwargs={} response_metadata={} id='543d17e5-8e65-4df0-8f9d-2428044a237b'",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "AIMessage",
                    "content": "content='' additional_kwargs={'tool_calls': [{'id': 'call_sGrZMqggHH89OyGlpGjK15OV', 'function': {'arguments': '{\"basename\":\"README.md\"}', 'name': 'find_file_node_with_basename'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 40208, 'total_tokens': 40227, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 39296}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--26ce42ef-8b1d-48b8-8788-4554fb8f3916-0' tool_calls=[{'name': 'find_file_node_with_basename', 'args': {'basename': 'README.md'}, 'id': 'call_sGrZMqggHH89OyGlpGjK15OV', 'type': 'tool_call'}] usage_metadata={'input_tokens': 40208, 'output_tokens': 19, 'total_tokens': 40227, 'input_token_details': {'audio': 0, 'cache_read': 39296}, 'output_token_details': {'audio': 0, 'reasoning': 0}}",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "ToolMessage",
                    "content": "content=\"Result 1:\\nFileNode: {'basename': 'README.md', 'relative_path': 'README.md', 'node_id': 4142745}\\n\\n\\nResult 2:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-yarn-tests/README.md', 'node_id': 4150818}\\n\\n\\nResult 3:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-test-utils-parent/flink-migration-test-utils/README.md', 'node_id': 4248694}\\n\\n\\nResult 4:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-table/README.md', 'node_id': 4256080}\\n\\n\\nResult 5:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-table/flink-table-planner/README.md', 'node_id': 4371807}\\n\\n\\nResult 6:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-runtime-web/README.md', 'node_id': 4901010}\\n\\n\\nResult 7:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-python/README.md', 'node_id': 5617655}\\n\\n\\nResult 8:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-python/apache-flink-libraries/README.md', 'node_id': 5709492}\\n\\n\\nResult 9:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/README.md', 'node_id': 5867943}\\n\\n\\nResult 10:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/README.md', 'node_id': 5871276}\\n\\n\\nResult 11:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/README.md', 'node_id': 5881602}\\n\\n\\nResult 12:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/README.md', 'node_id': 5885771}\\n\\n\\nResult 13:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/README.md', 'node_id': 5885774}\\n\\n\\nResult 14:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-docs/README.md', 'node_id': 5910092}\\n\\n\\nResult 15:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/README.md', 'node_id': 6149611}\\n\\n\\nResult 16:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-test/README.md', 'node_id': 6149615}\\n\\n\\nResult 17:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-production/README.md', 'node_id': 6149940}\\n\\n\\nResult 18:\\nFileNode: {'basename': 'README.md', 'relative_path': 'docs/README.md', 'node_id': 6152186}\" name='find_file_node_with_basename' id='e22a328b-0ca9-4405-99b2-983ace71a94c' tool_call_id='call_sGrZMqggHH89OyGlpGjK15OV' artifact=[{'FileNode': {'basename': 'README.md', 'relative_path': 'README.md', 'node_id': 4142745}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-yarn-tests/README.md', 'node_id': 4150818}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-test-utils-parent/flink-migration-test-utils/README.md', 'node_id': 4248694}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-table/README.md', 'node_id': 4256080}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-table/flink-table-planner/README.md', 'node_id': 4371807}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-runtime-web/README.md', 'node_id': 4901010}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-python/README.md', 'node_id': 5617655}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-python/apache-flink-libraries/README.md', 'node_id': 5709492}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/README.md', 'node_id': 5867943}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/README.md', 'node_id': 5871276}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/README.md', 'node_id': 5881602}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/README.md', 'node_id': 5885771}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/README.md', 'node_id': 5885774}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-docs/README.md', 'node_id': 5910092}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/README.md', 'node_id': 6149611}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-test/README.md', 'node_id': 6149615}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-production/README.md', 'node_id': 6149940}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'docs/README.md', 'node_id': 6152186}}]",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "AIMessage",
                    "content": "content='' additional_kwargs={'tool_calls': [{'id': 'call_TOeVGItfpfDAAz58iRShEHj8', 'function': {'arguments': '{\"basename\":\"README.md\"}', 'name': 'find_file_node_with_basename'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 40208, 'total_tokens': 40227, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 40192}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--7b6bf8fe-c6b4-4ff6-8e60-3856dc50da52-0' tool_calls=[{'name': 'find_file_node_with_basename', 'args': {'basename': 'README.md'}, 'id': 'call_TOeVGItfpfDAAz58iRShEHj8', 'type': 'tool_call'}] usage_metadata={'input_tokens': 40208, 'output_tokens': 19, 'total_tokens': 40227, 'input_token_details': {'audio': 0, 'cache_read': 40192}, 'output_token_details': {'audio': 0, 'reasoning': 0}}",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "ToolMessage",
                    "content": "content=\"Result 1:\\nFileNode: {'basename': 'README.md', 'relative_path': 'README.md', 'node_id': 4142745}\\n\\n\\nResult 2:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-yarn-tests/README.md', 'node_id': 4150818}\\n\\n\\nResult 3:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-test-utils-parent/flink-migration-test-utils/README.md', 'node_id': 4248694}\\n\\n\\nResult 4:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-table/README.md', 'node_id': 4256080}\\n\\n\\nResult 5:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-table/flink-table-planner/README.md', 'node_id': 4371807}\\n\\n\\nResult 6:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-runtime-web/README.md', 'node_id': 4901010}\\n\\n\\nResult 7:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-python/README.md', 'node_id': 5617655}\\n\\n\\nResult 8:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-python/apache-flink-libraries/README.md', 'node_id': 5709492}\\n\\n\\nResult 9:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/README.md', 'node_id': 5867943}\\n\\n\\nResult 10:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/README.md', 'node_id': 5871276}\\n\\n\\nResult 11:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/README.md', 'node_id': 5881602}\\n\\n\\nResult 12:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/README.md', 'node_id': 5885771}\\n\\n\\nResult 13:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/README.md', 'node_id': 5885774}\\n\\n\\nResult 14:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-docs/README.md', 'node_id': 5910092}\\n\\n\\nResult 15:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/README.md', 'node_id': 6149611}\\n\\n\\nResult 16:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-test/README.md', 'node_id': 6149615}\\n\\n\\nResult 17:\\nFileNode: {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-production/README.md', 'node_id': 6149940}\\n\\n\\nResult 18:\\nFileNode: {'basename': 'README.md', 'relative_path': 'docs/README.md', 'node_id': 6152186}\" name='find_file_node_with_basename' id='fd5e5fb3-c796-412a-9c93-1eaf21b0b22c' tool_call_id='call_TOeVGItfpfDAAz58iRShEHj8' artifact=[{'FileNode': {'basename': 'README.md', 'relative_path': 'README.md', 'node_id': 4142745}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-yarn-tests/README.md', 'node_id': 4150818}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-test-utils-parent/flink-migration-test-utils/README.md', 'node_id': 4248694}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-table/README.md', 'node_id': 4256080}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-table/flink-table-planner/README.md', 'node_id': 4371807}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-runtime-web/README.md', 'node_id': 4901010}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-python/README.md', 'node_id': 5617655}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-python/apache-flink-libraries/README.md', 'node_id': 5709492}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/README.md', 'node_id': 5867943}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/README.md', 'node_id': 5871276}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/README.md', 'node_id': 5881602}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/README.md', 'node_id': 5885771}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/README.md', 'node_id': 5885774}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-docs/README.md', 'node_id': 5910092}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/README.md', 'node_id': 6149611}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-test/README.md', 'node_id': 6149615}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-production/README.md', 'node_id': 6149940}}, {'FileNode': {'basename': 'README.md', 'relative_path': 'docs/README.md', 'node_id': 6152186}}]",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "AIMessage",
                    "content": "content='' additional_kwargs={'tool_calls': [{'id': 'call_MFV52jHrUObp6mRxVSLNzfGD', 'function': {'arguments': '{\"basename\":\"README.md\"}', 'name': 'preview_file_content_with_basename'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 40208, 'total_tokens': 40227, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 40192}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--c3d8edd3-b4cc-4bd4-b1e3-8f711b9a4cdf-0' tool_calls=[{'name': 'preview_file_content_with_basename', 'args': {'basename': 'README.md'}, 'id': 'call_MFV52jHrUObp6mRxVSLNzfGD', 'type': 'tool_call'}] usage_metadata={'input_tokens': 40208, 'output_tokens': 19, 'total_tokens': 40227, 'input_token_details': {'audio': 0, 'cache_read': 40192}, 'output_token_details': {'audio': 0, 'reasoning': 0}}",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "ToolMessage",
                    "content": "content='Result 1:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'README.md\\', \\'node_id\\': 4142745}\\npreview: # Apache Flink\\n\\nApache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.\\n\\nLearn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n\\n\\n### Features\\n\\n* A streaming-first runtime that supports both batch processing and data streaming programs\\n\\n* Elegant and fluent APIs in Java\\n\\n* A runtime that supports very high throughput and low event latency at the same time\\n\\n* Support for *event time* and *out-of-order* processing in the DataStream API, based on the *Dataflow Model*\\n\\n* Flexible windowing (time, count, sessions, custom triggers) across different time semantics (event time, processing time)\\n\\n* Fault-tolerance with *exactly-once* processing guarantees\\n\\n* Natural back-pressure in streaming programs\\n\\n* Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming)\\n\\n* Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms\\n\\n* Compatibility layers for Apache Hadoop MapReduce\\n\\n* Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem\\n\\n\\n### Streaming Example\\n```java\\n// pojo class WordWithCount\\npublic class WordWithCount {\\n    public String word;\\n    public int count;\\n\\n    public WordWithCount() {}\\n    \\n    public WordWithCount(String word, int count) {\\n        this.word = word;\\n        this.count = count;\\n    }\\n}\\n\\n// main method\\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\nDataStreamSource<String> text = env.socketTextStream(host, port);\\nDataStream<WordWithCount> windowCounts = text\\n    .flatMap(\\n        (FlatMapFunction<String, String>) (line, collector) \\n            -> Arrays.stream(line.split(\"\\\\\\\\s\")).forEach(collector::collect)\\n    ).returns(String.class)\\n    .map(word -> new WordWithCount(word, 1)).returns(TypeInformation.of(WordWithCount.class))\\n    .keyBy(wordWithCnt -> wordWithCnt.word)\\n    .window(TumblingProcessingTimeWindows.of(Duration.ofSeconds(5)))\\n    .sum(\"count\").returns(TypeInformation.of(WordWithCount.class));\\n\\nwindowCounts.print();\\nenv.execute();\\n}\\n```\\n\\n### Batch Example\\n```java\\n// pojo class WordWithCount\\npublic class WordWithCount {\\n    public String word;\\n    public int count;\\n\\n    public WordWithCount() {}\\n\\n    public WordWithCount(String word, int count) {\\n        this.word = word;\\n        this.count = count;\\n    }\\n}\\n\\n// main method\\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\nenv.setRuntimeMode(RuntimeExecutionMode.BATCH);\\nFileSource<String> source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), new Path(\"MyInput.txt\")).build();\\nDataStreamSource<String> text = env.fromSource(source, WatermarkStrategy.noWatermarks(), \"MySource\");\\nDataStream<WordWithCount> windowCounts = text\\n        .flatMap((FlatMapFunction<String, String>) (line, collector) -> Arrays\\n                .stream(line.split(\"\\\\\\\\s\"))\\n                .forEach(collector::collect)).returns(String.class)\\n        .map(word -> new WordWithCount(word, 1)).returns(TypeInformation.of(WordWithCount.class))\\n        .keyBy(wordWithCount -> wordWithCount.word)\\n        .sum(\"count\").returns(TypeInformation.of(WordWithCount.class));\\n\\nwindowCounts.print();\\nenv.execute();\\n```\\n\\n\\n\\n## Building Apache Flink from Source\\n\\nPrerequisites for building Flink:\\n\\n* Unix-like environment (we use Linux, Mac OS X, Cygwin, WSL)\\n* Git\\n* Maven (we require version 3.8.6)\\n* Java (version 11, 17, or 21)\\n\\n### Basic Build Instructions\\n\\nFirst, clone the repository:\\n\\n```\\ngit clone https://github.com/apache/flink.git\\ncd flink\\n```\\n\\nThen, choose one of the following commands based on your preferred Java version:\\n\\n**For Java 11**\\n\\n```\\n./mvnw clean package -DskipTests -Djdk11 -Pjava11-target\\n```\\n\\n**For Java 17 (Default)**\\n\\n```\\n./mvnw clean package -DskipTests -Djdk17 -Pjava17-target\\n```\\n\\n**For Java 21**\\n\\n```\\n./mvnw clean package -DskipTests -Djdk21 -Pjava21-target\\n```\\n\\nThe build process will take approximately 10 minutes to complete.\\nFlink will be installed in `build-target`.\\n\\n### Notes\\n\\n* Make sure your JAVA_HOME environment variable points to the correct JDK version\\n* The build command uses Maven wrapper (mvnw) which ensures the correct Maven version is used\\n* The -DskipTests flag skips running tests to speed up the build process\\n* Each Java version requires its corresponding profile (-Pjava<version>-target) and JDK flag (-Djdk<version>)\\n\\n## Developing Flink\\n\\nThe Flink committers use IntelliJ IDEA to develop the Flink codebase.\\nWe recommend IntelliJ IDEA for developing projects that involve Scala code.\\n\\nMinimal requirements for an IDE are:\\n* Support for Java and Scala (also mixed projects)\\n* Support for Maven with Java and Scala\\n\\n\\n### IntelliJ IDEA\\n\\nThe IntelliJ IDE supports Maven out of the box and offers a plugin for Scala development.\\n\\n* IntelliJ download: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)\\n* IntelliJ Scala Plugin: [https://plugins.jetbrains.com/plugin/?id=1347](https://plugins.jetbrains.com/plugin/?id=1347)\\n\\nCheck out our [Setting up IntelliJ](https://nightlies.apache.org/flink/flink-docs-master/flinkDev/ide_setup.html#intellij-idea) guide for details.\\n\\n### Eclipse Scala IDE\\n\\n**NOTE:** From our experience, this setup does not work with Flink\\ndue to deficiencies of the old Eclipse version bundled with Scala IDE 3.0.3 or\\ndue to version incompatibilities with the bundled Scala version in Scala IDE 4.4.1.\\n\\n**We recommend to use IntelliJ instead (see above)**\\n\\n## Support\\n\\nDon’t hesitate to ask!\\n\\nContact the developers and community on the [mailing lists](https://flink.apache.org/community.html#mailing-lists) if you need any help.\\n\\n[Open an issue](https://issues.apache.org/jira/browse/FLINK) if you find a bug in Flink.\\n\\n\\n## Documentation\\n\\nThe documentation of Apache Flink is located on the website: [https://flink.apache.org](https://flink.apache.org)\\nor in the `docs/` directory of the source code.\\n\\n\\n## Fork and Contribute\\n\\nThis is an active open-source project. We are always open to people who want to use the system or contribute to it.\\nContact us if you are looking for implementation tasks that fit your skills.\\nThis article describes [how to contribute to Apache Flink](https://flink.apache.org/contributing/how-to-contribute.html).\\n\\n## Externalized Connectors\\n\\nMost Flink connectors have been externalized to individual repos under the [Apache Software Foundation](https://github.com/apache):\\n\\n* [flink-connector-aws](https://github.com/apache/flink-connector-aws)\\n* [flink-connector-cassandra](https://github.com/apache/flink-connector-cassandra)\\n* [flink-connector-elasticsearch](https://github.com/apache/flink-connector-elasticsearch)\\n* [flink-connector-gcp-pubsub](https://github.com/apache/flink-connector-gcp-pubsub)\\n* [flink-connector-hbase](https://github.com/apache/flink-connector-hbase)\\n* [flink-connector-hive](https://github.com/apache/flink-connector-hive)\\n* [flink-connector-jdbc](https://github.com/apache/flink-connector-jdbc)\\n* [flink-connector-kafka](https://github.com/apache/flink-connector-kafka)\\n* [flink-connector-mongodb](https://github.com/apache/flink-connector-mongodb)\\n* [flink-connector-opensearch](https://github.com/apache/flink-connector-opensearch)\\n* [flink-connector-prometheus](https://github.com/apache/flink-connector-prometheus)\\n* [flink-connector-pulsar](https://github.com/apache/flink-connector-pulsar)\\n* [flink-connector-rabbitmq](https://github.com/apache/flink-connector-rabbitmq)\\n\\n## About\\n\\nApache Flink is an open source project of The Apache Software Foundation (ASF).\\nThe Apache Flink project originated from the [Stratosphere](http://stratosphere.eu) research project.\\n\\n\\nResult 2:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-yarn-tests/README.md\\', \\'node_id\\': 4150818}\\npreview: # Flink YARN tests\\n\\n`flink-yarn-test` collects test cases which are deployed to a local Apache Hadoop YARN cluster. \\nThere are several things to consider when running these tests locally:\\n\\n* `YarnTestBase` spins up a `MiniYARNCluster`. This cluster spawns processes outside of the IDE\\'s JVM \\n  to run the workers on. `JAVA_HOME` needs to be set to make this work.\\n* The Flink cluster within each test is deployed using the `flink-dist` binaries. Any changes made \\n  to the code will only take effect after rebuilding the `flink-dist` module.\\n* Each `YARN*ITCase` will have a local working directory for resources like logs to be stored. These \\n  working directories are located in `flink-yarn-tests/target/` (see \\n  `find flink-yarn-tests/target -name \"*.err\" -or -name \"*.out\"` for the test\\'s output).\\n\\n\\nResult 3:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-test-utils-parent/flink-migration-test-utils/README.md\\', \\'node_id\\': 4248694}\\npreview: # Add State Migration Tests\\n\\nThis module collects tools that help to generate test data for the state migration tests.\\n\\nThe following dependency needs to be added to the module\\'s Maven config in case a\\nmigration test is meant to be added to that module:\\n\\n```xml\\n<dependency>\\n    <groupId>org.apache.flink</groupId>\\n    <artifactId>flink-migration-test-utils</artifactId>\\n    <version>${project.version}</version>\\n    <scope>test</scope>\\n</dependency>\\n```\\n\\nand the following profile\\n\\n```xml\\n<profile>\\n    <id>generate-migration-test-data</id>\\n    <build>\\n        <plugins>\\n            <plugin>\\n                <artifactId>maven-antrun-plugin</artifactId>\\n                <executions>\\n                    <execution>\\n                        <id>generate-migration-test-data</id>\\n                        <phase>package</phase>\\n                        <goals>\\n                            <goal>run</goal>\\n                        </goals>\\n                        <configuration>\\n                            <target>\\n                                <condition property=\"optional.classes\" value=\"--classes \\'${generate.classes}\\'\"\\n                                           else=\"\">\\n                                    <isset property=\"generate.classes\"/>\\n                                </condition>\\n                                <condition property=\"optional.prefixes\"\\n                                           value=\"--prefixes \\'${generate.prefixes}\\'\" else=\"\">\\n                                    <isset property=\"generate.prefixes\"/>\\n                                </condition>\\n                                <java classname=\"org.apache.flink.test.migration.MigrationTestsSnapshotGenerator\"\\n                                      fork=\"true\" failonerror=\"true\" dir=\"${project.basedir}\">\\n                                    <classpath refid=\"maven.test.classpath\"/>\\n                                    <arg value=\"--dir\"/>\\n                                    <arg line=\"${project.basedir}\"/>\\n                                    <arg value=\"--version\"/>\\n                                    <arg value=\"${generate.version}\"/>\\n                                    <arg line=\"${optional.classes}\"/>\\n                                    <arg line=\"${optional.prefixes}\"/>\\n                                </java>\\n                            </target>\\n                        </configuration>\\n                    </execution>\\n                </executions>\\n            </plugin>\\n        </plugins>\\n    </build>\\n</profile>\\n```\\n\\nTo show the log during generating, add\\n\\n```\\nlogger.migration.name = org.apache.flink.test.migration\\nlogger.migration.level = INFO\\n```\\n\\nto the `log4j2-test.properties` of this module.\\n\\nThe state migration tests should satisfy\\n\\n1. The tests are named like `*(Test|ITCase).(java|scala)`.\\n2. The test class name is the same with the file name.\\n3. The test implements `org.apache.flink.test.util.MigrationTest` and the snapshots generator methods are labeled\\n   with `@SnapshotsGenerator` or `@ParameterizedSnapshotsGenerator`.\\n\\n# Generating Snapshots\\n\\nTo generate the snapshots for all the tests,\\nexecute from within the target version\\'s release branch:\\n\\n```shell\\nmvn clean package -Pgenerate-migration-test-data -Dgenerate.version=1.17 -nsu -Dfast -DskipTests\\n```\\n\\nThe version (`1.17` in the command above) should be replaced with the target one.\\n\\nBy default, it will search for the migration tests under `src/test/java` and `src/test/scala`. It is also supported\\nto change the default search paths or only generate for specific classes:\\n\\n```shell\\n# Change the default search paths\\nmvn clean package -Pgenerate-migration-test-data -Dgenerate.prefixes=xx,yy,zz -Dgenerate.version=1.17 -nsu -Dfast -DskipTests\\n\\n# Generate for the specified classes\\nmvn clean package -Pgenerate-migration-test-data -Dgenerate.classes=class1,class2 -Dgenerate.version=1.17 -nsu -Dfast -DskipTests\\n```\\n\\n\\nResult 4:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-table/README.md\\', \\'node_id\\': 4256080}\\npreview: # Table API & SQL\\n\\nApache Flink features two relational APIs - the Table API and SQL - for unified stream and batch processing. \\nThe Table API is a language-integrated query API for Java, Scala, and Python that allows the composition of queries from relational operators such as selection, filter, and join in a very intuitive way.\\n\\nThis documentation is intended for contributors of the table modules, and not for users. \\nIf you want to use Table API & SQL, check out the [documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/overview/).\\n\\n## Modules\\n\\n### Common\\n\\n* `flink-table-common`:\\n  * Type system definition\\n  * UDF stack and built-in function definitions\\n  * Internal data definitions\\n  * Extension points for catalogs, formats, connectors\\n  * Core APIs for extension points such as `Schema`\\n  * Utilities to deal with type system, internal data types and printing\\n\\n### API\\n\\n* `flink-table-api-java`: \\n  * Java APIs for Table API and SQL\\n  * Package `org.apache.flink.table.delegation`, which serves as entrypoint for all planner capabilities\\n* `flink-table-api-scala`: Scala APIs for Table API and SQL\\n* `flink-table-api-bridge-base`: Base classes for APIs to bridge between Table API and DataStream API\\n* `flink-table-api-java-bridge`: \\n  * Java APIs to bridge between Table API and DataStream API\\n  * Connectors that are developed using DataStream API, usually need to depend only on this module.\\n* `flink-table-api-scala-bridge`: Scala APIs to bridge between Table API and DataStream API\\n* `flink-table-api-java-uber`: \\n  * Uber JAR bundling `flink-table-common` and all the Java API modules, including the bridging to DataStream API and 3rd party dependencies.\\n  * This module is intended to be used by the flink-dist, rather than from the users directly.\\n* `flink-table-calcite-bridge`: Calcite dependencies for writing planner plugins (e.g. SQL dialects) that interact with Calcite API\\n\\n### Runtime\\n\\n* `flink-table-code-splitter`: Tool to split generated Java code so that each method does not exceed the limit of 64KB.\\n* `flink-table-runtime`:\\n  * Operator implementations\\n  * Built-in functions implementations\\n  * Type system implementation, including readers/writers, converters and utilities\\n  * Raw format\\n  * The produced jar includes all the classes from this module and `flink-table-code-splitter`, including 3rd party dependencies\\n\\n### Parser and planner\\n\\n* `flink-sql-parser`: Default ANSI SQL parser implementation\\n* `flink-table-planner`:\\n  * AST and Semantic tree\\n  * SQL validator\\n  * Query planner, optimizer and rules implementation\\n  * Code generator\\n  * The produced jar includes all the classes from this module together with the two parsers, including 3rd party dependencies (excluding Scala dependencies).\\n* `flink-table-planner-loader-bundle` Bundles `flink-table-planner`, including Scala dependencies.\\n* `flink-table-planner-loader`: Loader for `flink-table-planner` that loads the planner and it\\'s Scala dependencies in a separate classpath using `flink-table-planner-loader-bundle`, isolating the Scala version used to compile the planner.\\n\\n### SQL client\\n\\n* `flink-sql-client`: CLI tool to submit queries to a Flink cluster\\n\\n### Testing\\n\\n* `flink-table-test-utils`: Brings in transitively all the dependencies you need to execute Table pipelines and provides some test utilities such as assertions, mocks and test harnesses.\\n\\n### Notes\\n\\nNo module except `flink-table-planner` should depend on `flink-table-runtime` in production classpath,\\nno module except `flink-table-planner-loader` should depend on `flink-table-planner-loader-bundle` in production classpath,\\nand similarly no module should depend on `flink-table-planner` or `flink-table-planner-loader` in production classpath.\\nFor testing, you should depend on `flink-table-planner-loader` and `flink-table-runtime`.\\nThese are already shipped by the Flink distribution.\\n\\n\\nResult 5:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-table/flink-table-planner/README.md\\', \\'node_id\\': 4371807}\\npreview: # Table Planner\\n\\nThis module connects Table/SQL API and runtime. It is responsible for translating and optimizing a table program into a Flink pipeline. \\nFor user documentation, check the [table documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/overview/).\\n\\nThis README contains some development info for table planner contributors.\\n\\n## Immutables for rules\\n\\nCalcite\\'s `@ImmutableBeans.Property` has been removed in Calcite 1.28.0.\\n\\nSince Flink 1.17, planner rules use [Immutables](http://immutables.github.io/) to generate immutable\\nclasses for rule configs based on the provided interface. A config should be annotated with `@Value.Immutable`.\\nIn case the config is a nested class, the enclosing one should be annotated with `@Value.Enclosing`.\\n\\nOnce a new rule config is written and annotated, compile the module to generate the immutable class\\nfor that config. Generated code will be placed in `target/generated-sources/annotations`. The config\\ncan then be instantiated with the help of the generated immutable class like `Immutable<EnclosingClassName>.<ConfigClassName>`.\\n\\nIn case of issues, please double-check if a required generated class is present.\\nAs an example have a look at `org.apache.flink.table.planner.plan.rules.logical.EventTimeTemporalJoinRewriteRule`.\\nSee also `org.apache.calcite.plan.RelRule` for detailed explanation from Calcite.\\n\\n## Json Plan unit tests\\n\\nUnit tests verifying the JSON plan changes (e.g. Java tests in `org.apache.flink.table.planner.plan.nodes.exec.stream`) \\ncan regenerate all the files setting the environment variable `PLAN_TEST_FORCE_OVERWRITE=true`.\\n\\n\\nResult 6:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-runtime-web/README.md\\', \\'node_id\\': 4901010}\\npreview: <!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\"License\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n-->\\n\\n# Apache Flink Web Dashboard\\n\\nThe web dashboard is work in progress towards the new Flink runtime monitor. In particular, it will\\nprovide the following missing features of the current web dashboard:\\n\\n - Live progress monitoring (via live accumulators)\\n - A graph view of the program, as it is executed.\\n - A REST style API to access the status of individual jobs.\\n - A more modular design\\n\\nThe dashboard listens at `http://localhost:8081`.\\n\\nThe new web dashboard is work in progress. It starts an HTTP server (by default at port 8081)\\nthat serves the new web pages and additional background requests.\\n\\n## Server Backend\\n\\nThe server side of the dashboard is implemented using [Netty](http://netty.io) with\\n[Netty Router](https://github.com/sinetja/netty-router) for REST paths.\\nThe framework has very lightweight dependencies.\\n\\nThe code is regular Java code built via Maven. To add additional request handlers, follow the\\nexample of the `org.apache.flink.runtime.webmonitor.handlers.JobSummaryHandler`.\\n\\n\\n## Dashboard Frontend \\n\\nThe web dashboard is implemented using *Angular*. The dashboard build infrastructure uses *node.js*.\\n\\n\\n### Preparing the Build Environment\\n\\nDepending on your version of Linux, Windows or MacOS, you may need to manually install *node.js*\\n\\n\\n\\n#### Ubuntu Linux\\n\\nInstall *node.js* by following [these instructions](https://nodejs.org/en/download/).\\n\\nVerify that the installed version is at least *18.19.1*, via `node --version`.\\n\\n\\n#### MacOS\\n\\nFirst install *brew* by following [these instructions](http://brew.sh/).\\n\\nInstall *node.js* via:\\n\\n```\\nbrew install node\\n```\\n\\n### Building\\n\\nThe build process downloads all requires libraries via the *node.js* package management tool (*npm*)\\nThe final build tool is *@angular/cli*.\\n\\n```\\ncd flink-runtime-web/web-dashboard\\nnpm install\\nnpm run build\\n```\\n\\nThe dashboard code is under `/src`. The result of the build process is under `/web`.\\n\\n### Developing\\n\\nWhen developing the dashboard, every change needs to recompile the files and update the server:\\n\\n```\\ncd flink-runtime-web/web-dashboard\\nnpm run build\\ncd ../../flink-dist\\nmvn -DskipTests clean package\\n```\\n\\nTo simplify continuous development, one can use a *standalone proxy server*, together with automatic\\nre-compilation:\\n\\n1. Start the proxy server via `npm run proxy` (You can modify the proxy target in the `proxy.conf.json`, the default proxy target is `localhost:8081`)\\n2. Access the dashboard at [`http://localhost:4200`](http://localhost:4200)\\n\\n### CodeStyle & Lint\\n\\n> Tips: Before committing, make sure you\\'ve run `npm run lint-staged` and that no errors are reported.\\n\\n```bash\\n$ npm run lint\\n```\\n\\n### How to update the NOTICE\\n> Tip: If you\\'ve made a modification to any of the dependencies, make sure you\\'ve run `dev/generate_notice` before committing to update the `NOTICE` file.\\n\\n```bash\\n./web-dashboard/dev/generate_notice.sh\\n```\\n\\n### Dependency\\n\\n- Framework: [Angular](https://angular.dev)\\n- CLI Tools: [Angular CLI](https://angular.dev/tools/cli)\\n- UI Components: [NG-ZORRO](https://github.com/NG-ZORRO/ng-zorro-antd)\\n\\n\\nResult 7:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-python/README.md\\', \\'node_id\\': 5617655}\\npreview: # Apache Flink\\n\\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\\n\\nLearn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n\\n## Python Packaging\\n\\nPyFlink is a Python API for Apache Flink that allows you to build scalable batch and streaming workloads,\\nsuch as real-time data processing pipelines, large-scale exploratory data analysis, Machine Learning (ML)\\npipelines and ETL processes. If you’re already familiar with Python and libraries such as Pandas,\\nthen PyFlink makes it simpler to leverage the full capabilities of the Flink ecosystem.\\nDepending on the level of abstraction you need, there are two different APIs that can be used in PyFlink: PyFlink Table API and PyFlink DataStream API.\\n\\nThe PyFlink Table API allows you to write powerful relational queries in a way that is similar to\\nusing SQL or working with tabular data in Python. You can find more information about it via the tutorial\\n[https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/table_api_tutorial/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/table_api_tutorial/)\\n\\nThe PyFlink DataStream API gives you lower-level control over the core building blocks of Flink,\\nstate and time, to build more complex stream processing use cases.\\nTutorial can be found at [https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/datastream_tutorial/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/datastream_tutorial/)\\n\\nYou can find more information via the documentation at [https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/)\\n\\nThe auto-generated Python docs can be found at [https://nightlies.apache.org/flink/flink-docs-stable/api/python/](https://nightlies.apache.org/flink/flink-docs-stable/api/python/)\\n\\n## Python Requirements\\n\\nApache Flink Python API depends on Py4J (currently version 0.10.9.7), CloudPickle (currently version 2.2.0), python-dateutil (currently version >=2.8.0,<3), Apache Beam (currently version >= 2.54.0, <= 2.61.0).\\n\\n## Development Notices\\n\\n### Protobuf Code Generation\\n\\nProtocol buffer is used in file `flink_fn_execution_pb2.py` and `flink_fn_execution_pb2.pyi`, the file is generated from `flink-fn-execution.proto`. Whenever `flink-fn-execution.proto` is updated, please re-generate `flink_fn_execution_pb2.py` and `flink_fn_execution_pb2.pyi` by executing:\\n\\n```\\npython pyflink/gen_protos.py\\n```\\n\\nPyFlink depends on the following libraries to execute the above script:\\n1. grpcio-tools (>=1.29.0,<=1.71.0)\\n2. setuptools (>=37.0.0)\\n3. pip (>=20.3)\\n\\n### Running Test Cases\\n\\nCurrently, we use tox and to verify the compatibility of the Flink Python API for multiple versions of Python and will integrate some useful plugins with tox, such as flake8.\\nWe can enter the directory where this README.md file is located and run test cases by executing\\n\\n```\\n./dev/lint-python.sh\\n```\\n\\nTo use your system uv environment, you can set `FLINK_UV_HOME` variable:\\n\\n```shell\\nexport FLINK_UV_HOME=$(dirname $(dirname $(which uv)))\\n```\\n\\nCreate a virtual environment:\\n```shell\\nuv venv pyflink_39 --python=3.9\\n```\\n\\nThen you can activate your environment and run tests, for example:\\n\\n```shell\\nsource pyflink_39/bin/activate\\nuv pip install --group dev\\n./dev/lint-python.sh\\n```\\n\\n\\nResult 8:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-python/apache-flink-libraries/README.md\\', \\'node_id\\': 5709492}\\npreview: # Apache Flink\\n\\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\\n\\nLearn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n\\n## Python Packaging\\n\\nThis package bundles [apache-flink](https://pypi.org/project/apache-flink/) needed libraries\\n\\n\\nResult 9:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/README.md\\', \\'node_id\\': 5867943}\\npreview: Example: Running a state machine for pattern detection\\n======================================================\\n\\nThis example illustrates a minimal roll-your-own event pattern detection scenario,\\nusing a simple state machine that is evaluated over the stream.\\n\\nWhile this example is much simpler and more manual than what the CEP library supports,\\nit illustrates the use of event processing and state management for a medium\\ncomplex scenario.\\n\\n**Scenario Description**\\n\\nEvents in streams are expected to occur in certain patterns. Any deviation from\\nthese patterns indicates an anomaly that the streaming system should recognize and that\\nshould trigger an alert.\\n\\nYou can, for example, think of events as being generated by network devices and services,\\nsuch as firewalls login-, and registration with an authentication service, etc. A deviation\\nfrom the expected pattern might indicate an intrusion detection.\\n\\nThe event patterns are tracked per interacting party (here simplified per source IP address)\\nand are validated by a state machine. The state machine\\'s states define what possible\\nevents may occur next, and what new states these events will result in.\\n\\nThe following diagram depicts the state machine used in this example.\\n\\n```\\n           +--<a>--> W --<b>--> Y --<e>---+\\n           |                    ^         |\\n   INITIAL-+                    |         |\\n           |                    |         +--> (Z) -----<g>---> TERM\\n           +--<c>--> X --<b>----+         |\\n                     |                    |\\n                     +--------<d>---------+\\n```\\n\\n**Example Program**\\n\\nThe main class of this example program is `org.apache.flink.streaming.examples.statemachine.StateMachineExample`.\\nThe core logic is in the `flatMap` function, which runs the state machines per IP address.\\n\\nThe streaming data flow is as shown below, where the source stream may come from either\\nan embedded data generator, or from a from a Kafka topic:\\n\\n```\\n [ stream partition 1] --> source --> partition -+---> flatMap(state machine) --> sink\\n                                            \\\\/\\n                                            /\\\\\\n [ stream partition 2] --> source --> partition -+---> flatMap(state machine) --> sink\\n```\\n\\n\\nResult 10:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-end-to-end-tests/README.md\\', \\'node_id\\': 5871276}\\npreview: # Flink End-to-End Tests\\n\\nThis module contains tests that verify end-to-end behaviour of Flink. \\n\\nThe tests defined in `run-nightly-tests.sh` are run by the CI system on every pull request\\nand push to master.\\n\\n\\n## Running Tests\\nYou can run all tests by executing\\n\\n```\\n$ FLINK_DIR=<flink dir> flink-end-to-end-tests/run-nightly-tests.sh\\n```\\n\\nwhere \\\\<flink dir\\\\> is a Flink distribution directory.\\n\\nYou can also run tests individually via\\n\\n```\\n$ FLINK_DIR=<flink dir> flink-end-to-end-tests/run-single-test.sh your_test.sh arg1 arg2\\n```\\n\\n**NOTICE**: Please _DON\\'T_ run the scripts with explicit command like ```sh run-nightly-tests.sh``` since ```#!/usr/bin/env bash``` is specified as the header of the scripts to assure flexibility on different systems.\\n\\n**NOTICE**: We do not recommend executing the nightly test script on production or personal desktop systems, as tests contained there might modify the environment (leftover processes, modification of system files, request for root permissions via sudo, ...).\\n\\n### Kubernetes test\\n\\nKubernetes test (test_kubernetes_embedded_job.sh) assumes a running minikube cluster.\\n\\n\\n## Writing Tests\\n\\nAs of November 2020, Flink has two broad types of end-to-end tests: Bash-based end-to-end tests, located in the `test-scripts/` directory and Java-based end-to-end tests, such as the `PrometheusReporterEndToEndITCase`. The community recommends writing new tests as Java tests, as we are planning to deprecate the bash-based tests in the long run.\\n\\n\\n### Examples\\nHave a look at `test_batch_wordcount.sh` for a very basic test and\\n`test_streaming_kafka010.sh` for a more involved example. Whenever possible, try\\nto put new functionality in `common.sh` so that it can be reused by other tests.\\n\\n### Adding a test case\\nIn order to add a new test case you need add it to `test-scripts/run-nightly-tests.sh`. Templates on how to add tests can be found in those respective files.\\n\\n_Note: If you want to parameterize your tests please do so by adding multiple test cases with parameters as arguments to the nightly / pre-commit test suites. This allows the test runner to do a cleanup in between each individual test and also to fail those tests individually._\\n\\n_Note: While developing a new test case make sure to enable bash\\'s error handling in `test-scripts/common.sh` by uncommenting `set -Eexuo pipefail` and commenting the current default `set` call. Once your test is implemented properly, add `set -Eeuo pipefail` on the very top of your test script (before any `common` script)._\\n\\n### Passing your test\\nA test is considered to have passed if it:\\n- has exit code 0\\n- there are no non-empty .out files (nothing was written to stdout / stderr by your Flink program)\\n- there are no exceptions in the log files\\n- there are no errors in the log files\\n\\n_Note: There is a whitelist for exceptions and errors that do not lead to failure, which can be found in the `check_logs_for_errors` and `check_logs_for_exceptions` in `test-scripts/common.sh`._\\n\\nPlease note that a previously supported pattern where you could assign a value the global variable `PASS` to have your tests fail **is not supported anymore**.\\n\\n### Cleanup\\nThe test runner performs a cleanup after each test case, which includes:\\n- Stopping the cluster\\n- Killing all task and job managers\\n- Reverting `conf` and `lib` dirs to default\\n- Cleaning up log and temp directories\\n\\nIn some cases your test is required to do some *additional* cleanup, for example shutting down external systems like Kafka or Elasticsearch. In this case you can register a function that will be called on test exit like this:\\n\\n```sh\\nfunction test_cleanup {\\n    # do your custom cleanup here\\n}\\n\\non_exit test_cleanup\\n```\\n\\n\\nResult 11:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/README.md\\', \\'node_id\\': 5881602}\\npreview: # Apache Hadoop Docker image with Kerberos enabled\\n\\nRequired versions\\n-----------------\\n\\n* JDK8\\n* Hadoop 2.10.2\\n\\nDefault Environment Variables\\n-----------------------------\\n\\n| Name                      | Value | Description                                                                                                                                     |\\n|---------------------------| ----  |-------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `KRB_REALM`               | `EXAMPLE.COM` | The Kerberos Realm, more information [here](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html#)                        |\\n| `DOMAIN_REALM`            | `example.com` | The Kerberos Domain Realm, more information [here](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html#)                 |\\n| `KERB_MASTER_KEY`         | `masterkey` | The Kerberos master database password, more information [here](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/admin_commands/kdb5_util.html#) |\\n| `KERBEROS_ADMIN_USER`     | `admin/admin` | The KDC admin user                                                                                                                              |\\n| `KERBEROS_ADMIN_PASSWORD` | `admin` | The KDC admin password                                                                                                                          |\\n\\nYou can simply define these variables in the `docker-compose.yml`.\\n\\nRun image\\n---------\\n\\n```\\ncd flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster\\nwget -O hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-2.10.2/hadoop-2.10.2.tar.gz\\ndocker compose build\\ndocker compose up\\n```\\n\\nUsage\\n-----\\n\\nGet the container name with `docker ps` and login to the container with\\n\\n```\\ndocker exec -it <container-name> /bin/bash\\n```\\n\\nTo obtain a Kerberos ticket, execute\\n\\n```\\nkinit -kt /home/hadoop-user/hadoop-user.keytab hadoop-user\\n```\\n\\nAfterwards you can use `hdfs` CLI like\\n\\n```\\nhdfs dfs -ls /\\n```\\n\\nKnown issues\\n------------\\n\\n### Unable to obtain Kerberos password\\n\\n#### Error\\ndocker compose up fails for the first time with the error\\n\\n```\\nLogin failure for nn/hadoop.docker.com@EXAMPLE.COM from keytab /etc/security/keytabs/nn.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user\\n```\\n\\n#### Solution\\n\\nStop the containers with `docker compose down` and start again with `docker compose up -d`.\\n\\n### Java Keystore\\n\\nIf the Keystore has been expired, then create a new `keystore.jks`:\\n\\n1. create private key\\n\\n```\\nopenssl genrsa -des3 -out server.key 1024\\n```\\n\\n2. create csr\\n\\n```\\nopenssl req -new -key server.key -out server.csr`\\n```\\n\\n3. remove passphrase in key\\n```\\ncp server.key server.key.org\\nopenssl rsa -in server.key.org -out server.key\\n```\\n\\n3. create self-signed cert\\n```\\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\\n```\\n\\n4. create JKS and import certificate\\n```\\nkeytool -import -keystore keystore.jks -alias CARoot -file server.crt`\\n```\\n\\n\\nResult 12:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-end-to-end-tests/flink-tpcds-test/README.md\\', \\'node_id\\': 5885771}\\npreview: # Flink [TPC-DS]( http://www.tpc.org/tpcds) test\\n\\nYou can run the Flink TPC-DS test in your local environment as following commands:\\n\\n```\\n$ cd <your_workspace_dir>/flink                                                                               # go to the Flink project   \\n$ mvn compile -DskipTests=true                                                                               # compile Flink source code\\n$ export FLINK_DIR=<your_workspace_dir>/flink/flink-dist/target/flink-${flink-version}-bin/flink-${flink-version}  # set Flink distribution directory, the ${flink-version} is the compiled Flink version\\n$ sh flink-end-to-end-tests/run-single-test.sh flink-end-to-end-tests/test-scripts/test_tpcds.sh               # run a TPC-DS benchmark\\n```\\n\\nThe default scale factor of TPC-DS benchmark in Flink is 1GB, Flink use this scale factor from validation purpose.\\n\\n\\nResult 13:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/README.md\\', \\'node_id\\': 5885774}\\npreview: # TPC-DS tool\\n\\nThis directory contains the necessary tools to run a [TPC-DS]( http://www.tpc.org/tpcds) benchmark.\\n\\n## data_generator.sh scripts\\n\\nScripts to download the TPC-DS data generator and generate test data, the supported OS including GNU/Linux OS and Mac OS.\\n```\\n data_generator.sh <generatorDir> <scaleFactor> <outputDataDir> <commonScriptsDir>\\n```\\n* generator_dir: directory to save TPC-DS data generator.\\n* scale_factor: scale factor indicates raw data size in GB, we use the validation scale factor 1 (1G) by default.\\n* data_dir: directory to save TPC-DS test data.\\n* common_scripts_dir: directory which contains some common scripts for Flink end to end tests, the retry scripts `retry_times_with_backoff_and_cleanup()` will be used when download the TPC-DS data generator from the Internet.\\n\\nThe scripts is internally used in Flink end to end tests, give an example to specific how the scripts works:\\n```\\n$ cd <your_workspace_dir>/flink                                                                                   # go to the Flink project\\n$ mvn compile -DskipTests=true                                                                                   # compile the Flink source code\\n$ export FLINK_DIR=<your_workspace_dir>/flink/flink-dist/target/flink-${flink-version}-bin/flink-${flink-version}      # set Flink distribution directory, the ${flink-version} is the compiled Flink version\\n$ export END_TO_END_DIR=<your_workspace_dir>/flink/flink-end-to-end-tests                                          # set end to end tests directory\\n$ mkdir -p <your_workspace_dir>/dir_to_save_genarator\\n$ mkdir -p <your_workspace_dir>/dir_to_save_data\\n$ cd <your_workspace_dir>/flink/flink-end-to-end-tests/flink-tpcds-test/tpcds-tool\\n$ sh data_generator.sh <your_workspace_dir>/dir_to_save_genarator 1 <your_workspace_dir>/dir_to_save_data <your_workspace_dir>/flink/flink-end-to-end-tests/test-scripts\\n```\\nThe downloaded generator will be saved to `<your_workspace_dir>/dir_to_save_genarator`, the generated data will be saved to `<your_workspace_dir>/dir_to_save_data`.\\n\\n**NOTE:** If you want to run a TPC-DS benchmark in Flink, please read `flink-tpcds-test/README.md` for more information.\\n    \\n## query directory\\n\\nDirectory that contains all TPC-DS queries (103 queries in total), the corresponding data scale factor is 1G, these queries come from TPC-DS standard specification v2.11.0 [1].\\n\\n**NOTE:** The scale factor of queries should match with test data\\'s scale factor.\\n    \\n## answer_set directory\\n\\nDirectory that contains all answers for per query, the answer content of query may has different order according to the system\\'s null order.\\n\\n* (1) Flink SQL keeps null first in ASC order, keeps null last in DESC order, we choose corresponding answer file when compare the answer.\\n* (2) For query 8, 14a, 18, 70 and 77, the decimal precision of answer set is too low, we compare query result with SQL server\\'s answer, they can strictly match.\\n   \\n ## Reference   \\n[1] http://www.tpc.org/tpc_documents_current_versions/pdf/tpc-ds_v2.11.0.pdf\\n\\n\\nResult 14:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative_path\\': \\'flink-docs/README.md\\', \\'node_id\\': 5910092}\\npreview: <!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\"License\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n-->\\n\\n# Documentation generators\\n\\nThis module contains generators that create HTML files directly from Flink\\'s source code.\\n\\n## REST API documentation\\n\\nThe `RestAPIDocGenerator`/`OpenApiSpecGenerator` can be used to generate a full reference of the REST API of a `RestServerEndpoint`. A separate file is generated for each endpoint.\\n\\nTo integrate a new endpoint into the generator\\n1. Add a new `DocumentingRestEndpoint` class to `RestAPIDocGenerator` that extends the new endpoint class\\n2. Add another call to `createHtmlFile` in `RestAPIDocGenerator#main`\\n3. Regenerate the documentation by running `mvn package -Dgenerate-rest-docs -pl flink-docs -am -nsu -DskipTests`\\n4. Integrate the generated file into the REST API documentation by adding `{{< generated/<file-name> }}` to the corresponding markdown file.\\n\\nThe documentation must be regenerated whenever\\n* a handler is added to/removed from a `RestServerEndpoint`\\n* any used `MessageHeaders` class or any referenced `RequestBody`, `ResponseBody`, `MessageParameters` or `MessageParameter` class is modified.\\n\\n## Configuration documentation\\n\\nThe `ConfigOptionsDocGenerator` can be used to generate a reference of `ConfigOptions`. By default, a separate file is generated for each `*Options` class found in `org.apache.flink.configuration` and `org.apache.flink.yarn.configuration`. The `@ConfigGroups` annotation can be used to generate multiple files from a single class.\\n\\nTo integrate an `*Options` class from another package, add another module-package argument pair to `ConfigurationOptionLocator#LOCATIONS`.\\n\\nThe files can be generated by running `mvn package -Dgenerate-config-docs -pl flink-docs -am -nsu -DskipTests -Pskip-webui-build`, and can be integrated into the documentation using `{{ include generated/<file-name> >}}`.\\n\\nThe documentation must be regenerated whenever\\n* an `*Options` class was added or removed\\n* a `ConfigOption` was added to or removed from an `*Options` class\\n* a `ConfigOption` was modified in any way.\\n\\n\\nResult 15:\\nFileNode: {\\'basename\\': \\'README.md\\', \\'relative... Output has been truncated becuase it is too long, please narrow down your query if you wish to see more' name='preview_file_content_with_basename' id='b71c4102-535d-40ed-9b1f-c7f70d661dcc' tool_call_id='call_MFV52jHrUObp6mRxVSLNzfGD' artifact=[{'FileNode': {'basename': 'README.md', 'relative_path': 'README.md', 'node_id': 4142745}, 'preview': '# Apache Flink\\n\\nApache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.\\n\\nLearn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n\\n\\n### Features\\n\\n* A streaming-first runtime that supports both batch processing and data streaming programs\\n\\n* Elegant and fluent APIs in Java\\n\\n* A runtime that supports very high throughput and low event latency at the same time\\n\\n* Support for *event time* and *out-of-order* processing in the DataStream API, based on the *Dataflow Model*\\n\\n* Flexible windowing (time, count, sessions, custom triggers) across different time semantics (event time, processing time)\\n\\n* Fault-tolerance with *exactly-once* processing guarantees\\n\\n* Natural back-pressure in streaming programs\\n\\n* Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming)\\n\\n* Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms\\n\\n* Compatibility layers for Apache Hadoop MapReduce\\n\\n* Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem\\n\\n\\n### Streaming Example\\n```java\\n// pojo class WordWithCount\\npublic class WordWithCount {\\n    public String word;\\n    public int count;\\n\\n    public WordWithCount() {}\\n    \\n    public WordWithCount(String word, int count) {\\n        this.word = word;\\n        this.count = count;\\n    }\\n}\\n\\n// main method\\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\nDataStreamSource<String> text = env.socketTextStream(host, port);\\nDataStream<WordWithCount> windowCounts = text\\n    .flatMap(\\n        (FlatMapFunction<String, String>) (line, collector) \\n            -> Arrays.stream(line.split(\"\\\\\\\\s\")).forEach(collector::collect)\\n    ).returns(String.class)\\n    .map(word -> new WordWithCount(word, 1)).returns(TypeInformation.of(WordWithCount.class))\\n    .keyBy(wordWithCnt -> wordWithCnt.word)\\n    .window(TumblingProcessingTimeWindows.of(Duration.ofSeconds(5)))\\n    .sum(\"count\").returns(TypeInformation.of(WordWithCount.class));\\n\\nwindowCounts.print();\\nenv.execute();\\n}\\n```\\n\\n### Batch Example\\n```java\\n// pojo class WordWithCount\\npublic class WordWithCount {\\n    public String word;\\n    public int count;\\n\\n    public WordWithCount() {}\\n\\n    public WordWithCount(String word, int count) {\\n        this.word = word;\\n        this.count = count;\\n    }\\n}\\n\\n// main method\\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\nenv.setRuntimeMode(RuntimeExecutionMode.BATCH);\\nFileSource<String> source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), new Path(\"MyInput.txt\")).build();\\nDataStreamSource<String> text = env.fromSource(source, WatermarkStrategy.noWatermarks(), \"MySource\");\\nDataStream<WordWithCount> windowCounts = text\\n        .flatMap((FlatMapFunction<String, String>) (line, collector) -> Arrays\\n                .stream(line.split(\"\\\\\\\\s\"))\\n                .forEach(collector::collect)).returns(String.class)\\n        .map(word -> new WordWithCount(word, 1)).returns(TypeInformation.of(WordWithCount.class))\\n        .keyBy(wordWithCount -> wordWithCount.word)\\n        .sum(\"count\").returns(TypeInformation.of(WordWithCount.class));\\n\\nwindowCounts.print();\\nenv.execute();\\n```\\n\\n\\n\\n## Building Apache Flink from Source\\n\\nPrerequisites for building Flink:\\n\\n* Unix-like environment (we use Linux, Mac OS X, Cygwin, WSL)\\n* Git\\n* Maven (we require version 3.8.6)\\n* Java (version 11, 17, or 21)\\n\\n### Basic Build Instructions\\n\\nFirst, clone the repository:\\n\\n```\\ngit clone https://github.com/apache/flink.git\\ncd flink\\n```\\n\\nThen, choose one of the following commands based on your preferred Java version:\\n\\n**For Java 11**\\n\\n```\\n./mvnw clean package -DskipTests -Djdk11 -Pjava11-target\\n```\\n\\n**For Java 17 (Default)**\\n\\n```\\n./mvnw clean package -DskipTests -Djdk17 -Pjava17-target\\n```\\n\\n**For Java 21**\\n\\n```\\n./mvnw clean package -DskipTests -Djdk21 -Pjava21-target\\n```\\n\\nThe build process will take approximately 10 minutes to complete.\\nFlink will be installed in `build-target`.\\n\\n### Notes\\n\\n* Make sure your JAVA_HOME environment variable points to the correct JDK version\\n* The build command uses Maven wrapper (mvnw) which ensures the correct Maven version is used\\n* The -DskipTests flag skips running tests to speed up the build process\\n* Each Java version requires its corresponding profile (-Pjava<version>-target) and JDK flag (-Djdk<version>)\\n\\n## Developing Flink\\n\\nThe Flink committers use IntelliJ IDEA to develop the Flink codebase.\\nWe recommend IntelliJ IDEA for developing projects that involve Scala code.\\n\\nMinimal requirements for an IDE are:\\n* Support for Java and Scala (also mixed projects)\\n* Support for Maven with Java and Scala\\n\\n\\n### IntelliJ IDEA\\n\\nThe IntelliJ IDE supports Maven out of the box and offers a plugin for Scala development.\\n\\n* IntelliJ download: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)\\n* IntelliJ Scala Plugin: [https://plugins.jetbrains.com/plugin/?id=1347](https://plugins.jetbrains.com/plugin/?id=1347)\\n\\nCheck out our [Setting up IntelliJ](https://nightlies.apache.org/flink/flink-docs-master/flinkDev/ide_setup.html#intellij-idea) guide for details.\\n\\n### Eclipse Scala IDE\\n\\n**NOTE:** From our experience, this setup does not work with Flink\\ndue to deficiencies of the old Eclipse version bundled with Scala IDE 3.0.3 or\\ndue to version incompatibilities with the bundled Scala version in Scala IDE 4.4.1.\\n\\n**We recommend to use IntelliJ instead (see above)**\\n\\n## Support\\n\\nDon’t hesitate to ask!\\n\\nContact the developers and community on the [mailing lists](https://flink.apache.org/community.html#mailing-lists) if you need any help.\\n\\n[Open an issue](https://issues.apache.org/jira/browse/FLINK) if you find a bug in Flink.\\n\\n\\n## Documentation\\n\\nThe documentation of Apache Flink is located on the website: [https://flink.apache.org](https://flink.apache.org)\\nor in the `docs/` directory of the source code.\\n\\n\\n## Fork and Contribute\\n\\nThis is an active open-source project. We are always open to people who want to use the system or contribute to it.\\nContact us if you are looking for implementation tasks that fit your skills.\\nThis article describes [how to contribute to Apache Flink](https://flink.apache.org/contributing/how-to-contribute.html).\\n\\n## Externalized Connectors\\n\\nMost Flink connectors have been externalized to individual repos under the [Apache Software Foundation](https://github.com/apache):\\n\\n* [flink-connector-aws](https://github.com/apache/flink-connector-aws)\\n* [flink-connector-cassandra](https://github.com/apache/flink-connector-cassandra)\\n* [flink-connector-elasticsearch](https://github.com/apache/flink-connector-elasticsearch)\\n* [flink-connector-gcp-pubsub](https://github.com/apache/flink-connector-gcp-pubsub)\\n* [flink-connector-hbase](https://github.com/apache/flink-connector-hbase)\\n* [flink-connector-hive](https://github.com/apache/flink-connector-hive)\\n* [flink-connector-jdbc](https://github.com/apache/flink-connector-jdbc)\\n* [flink-connector-kafka](https://github.com/apache/flink-connector-kafka)\\n* [flink-connector-mongodb](https://github.com/apache/flink-connector-mongodb)\\n* [flink-connector-opensearch](https://github.com/apache/flink-connector-opensearch)\\n* [flink-connector-prometheus](https://github.com/apache/flink-connector-prometheus)\\n* [flink-connector-pulsar](https://github.com/apache/flink-connector-pulsar)\\n* [flink-connector-rabbitmq](https://github.com/apache/flink-connector-rabbitmq)\\n\\n## About\\n\\nApache Flink is an open source project of The Apache Software Foundation (ASF).\\nThe Apache Flink project originated from the [Stratosphere](http://stratosphere.eu) research project.'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-yarn-tests/README.md', 'node_id': 4150818}, 'preview': '# Flink YARN tests\\n\\n`flink-yarn-test` collects test cases which are deployed to a local Apache Hadoop YARN cluster. \\nThere are several things to consider when running these tests locally:\\n\\n* `YarnTestBase` spins up a `MiniYARNCluster`. This cluster spawns processes outside of the IDE\\'s JVM \\n  to run the workers on. `JAVA_HOME` needs to be set to make this work.\\n* The Flink cluster within each test is deployed using the `flink-dist` binaries. Any changes made \\n  to the code will only take effect after rebuilding the `flink-dist` module.\\n* Each `YARN*ITCase` will have a local working directory for resources like logs to be stored. These \\n  working directories are located in `flink-yarn-tests/target/` (see \\n  `find flink-yarn-tests/target -name \"*.err\" -or -name \"*.out\"` for the test\\'s output).'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-test-utils-parent/flink-migration-test-utils/README.md', 'node_id': 4248694}, 'preview': '# Add State Migration Tests\\n\\nThis module collects tools that help to generate test data for the state migration tests.\\n\\nThe following dependency needs to be added to the module\\'s Maven config in case a\\nmigration test is meant to be added to that module:\\n\\n```xml\\n<dependency>\\n    <groupId>org.apache.flink</groupId>\\n    <artifactId>flink-migration-test-utils</artifactId>\\n    <version>${project.version}</version>\\n    <scope>test</scope>\\n</dependency>\\n```\\n\\nand the following profile\\n\\n```xml\\n<profile>\\n    <id>generate-migration-test-data</id>\\n    <build>\\n        <plugins>\\n            <plugin>\\n                <artifactId>maven-antrun-plugin</artifactId>\\n                <executions>\\n                    <execution>\\n                        <id>generate-migration-test-data</id>\\n                        <phase>package</phase>\\n                        <goals>\\n                            <goal>run</goal>\\n                        </goals>\\n                        <configuration>\\n                            <target>\\n                                <condition property=\"optional.classes\" value=\"--classes \\'${generate.classes}\\'\"\\n                                           else=\"\">\\n                                    <isset property=\"generate.classes\"/>\\n                                </condition>\\n                                <condition property=\"optional.prefixes\"\\n                                           value=\"--prefixes \\'${generate.prefixes}\\'\" else=\"\">\\n                                    <isset property=\"generate.prefixes\"/>\\n                                </condition>\\n                                <java classname=\"org.apache.flink.test.migration.MigrationTestsSnapshotGenerator\"\\n                                      fork=\"true\" failonerror=\"true\" dir=\"${project.basedir}\">\\n                                    <classpath refid=\"maven.test.classpath\"/>\\n                                    <arg value=\"--dir\"/>\\n                                    <arg line=\"${project.basedir}\"/>\\n                                    <arg value=\"--version\"/>\\n                                    <arg value=\"${generate.version}\"/>\\n                                    <arg line=\"${optional.classes}\"/>\\n                                    <arg line=\"${optional.prefixes}\"/>\\n                                </java>\\n                            </target>\\n                        </configuration>\\n                    </execution>\\n                </executions>\\n            </plugin>\\n        </plugins>\\n    </build>\\n</profile>\\n```\\n\\nTo show the log during generating, add\\n\\n```\\nlogger.migration.name = org.apache.flink.test.migration\\nlogger.migration.level = INFO\\n```\\n\\nto the `log4j2-test.properties` of this module.\\n\\nThe state migration tests should satisfy\\n\\n1. The tests are named like `*(Test|ITCase).(java|scala)`.\\n2. The test class name is the same with the file name.\\n3. The test implements `org.apache.flink.test.util.MigrationTest` and the snapshots generator methods are labeled\\n   with `@SnapshotsGenerator` or `@ParameterizedSnapshotsGenerator`.\\n\\n# Generating Snapshots\\n\\nTo generate the snapshots for all the tests,\\nexecute from within the target version\\'s release branch:\\n\\n```shell\\nmvn clean package -Pgenerate-migration-test-data -Dgenerate.version=1.17 -nsu -Dfast -DskipTests\\n```\\n\\nThe version (`1.17` in the command above) should be replaced with the target one.\\n\\nBy default, it will search for the migration tests under `src/test/java` and `src/test/scala`. It is also supported\\nto change the default search paths or only generate for specific classes:\\n\\n```shell\\n# Change the default search paths\\nmvn clean package -Pgenerate-migration-test-data -Dgenerate.prefixes=xx,yy,zz -Dgenerate.version=1.17 -nsu -Dfast -DskipTests\\n\\n# Generate for the specified classes\\nmvn clean package -Pgenerate-migration-test-data -Dgenerate.classes=class1,class2 -Dgenerate.version=1.17 -nsu -Dfast -DskipTests\\n```'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-table/README.md', 'node_id': 4256080}, 'preview': \"# Table API & SQL\\n\\nApache Flink features two relational APIs - the Table API and SQL - for unified stream and batch processing. \\nThe Table API is a language-integrated query API for Java, Scala, and Python that allows the composition of queries from relational operators such as selection, filter, and join in a very intuitive way.\\n\\nThis documentation is intended for contributors of the table modules, and not for users. \\nIf you want to use Table API & SQL, check out the [documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/overview/).\\n\\n## Modules\\n\\n### Common\\n\\n* `flink-table-common`:\\n  * Type system definition\\n  * UDF stack and built-in function definitions\\n  * Internal data definitions\\n  * Extension points for catalogs, formats, connectors\\n  * Core APIs for extension points such as `Schema`\\n  * Utilities to deal with type system, internal data types and printing\\n\\n### API\\n\\n* `flink-table-api-java`: \\n  * Java APIs for Table API and SQL\\n  * Package `org.apache.flink.table.delegation`, which serves as entrypoint for all planner capabilities\\n* `flink-table-api-scala`: Scala APIs for Table API and SQL\\n* `flink-table-api-bridge-base`: Base classes for APIs to bridge between Table API and DataStream API\\n* `flink-table-api-java-bridge`: \\n  * Java APIs to bridge between Table API and DataStream API\\n  * Connectors that are developed using DataStream API, usually need to depend only on this module.\\n* `flink-table-api-scala-bridge`: Scala APIs to bridge between Table API and DataStream API\\n* `flink-table-api-java-uber`: \\n  * Uber JAR bundling `flink-table-common` and all the Java API modules, including the bridging to DataStream API and 3rd party dependencies.\\n  * This module is intended to be used by the flink-dist, rather than from the users directly.\\n* `flink-table-calcite-bridge`: Calcite dependencies for writing planner plugins (e.g. SQL dialects) that interact with Calcite API\\n\\n### Runtime\\n\\n* `flink-table-code-splitter`: Tool to split generated Java code so that each method does not exceed the limit of 64KB.\\n* `flink-table-runtime`:\\n  * Operator implementations\\n  * Built-in functions implementations\\n  * Type system implementation, including readers/writers, converters and utilities\\n  * Raw format\\n  * The produced jar includes all the classes from this module and `flink-table-code-splitter`, including 3rd party dependencies\\n\\n### Parser and planner\\n\\n* `flink-sql-parser`: Default ANSI SQL parser implementation\\n* `flink-table-planner`:\\n  * AST and Semantic tree\\n  * SQL validator\\n  * Query planner, optimizer and rules implementation\\n  * Code generator\\n  * The produced jar includes all the classes from this module together with the two parsers, including 3rd party dependencies (excluding Scala dependencies).\\n* `flink-table-planner-loader-bundle` Bundles `flink-table-planner`, including Scala dependencies.\\n* `flink-table-planner-loader`: Loader for `flink-table-planner` that loads the planner and it's Scala dependencies in a separate classpath using `flink-table-planner-loader-bundle`, isolating the Scala version used to compile the planner.\\n\\n### SQL client\\n\\n* `flink-sql-client`: CLI tool to submit queries to a Flink cluster\\n\\n### Testing\\n\\n* `flink-table-test-utils`: Brings in transitively all the dependencies you need to execute Table pipelines and provides some test utilities such as assertions, mocks and test harnesses.\\n\\n### Notes\\n\\nNo module except `flink-table-planner` should depend on `flink-table-runtime` in production classpath,\\nno module except `flink-table-planner-loader` should depend on `flink-table-planner-loader-bundle` in production classpath,\\nand similarly no module should depend on `flink-table-planner` or `flink-table-planner-loader` in production classpath.\\nFor testing, you should depend on `flink-table-planner-loader` and `flink-table-runtime`.\\nThese are already shipped by the Flink distribution.\"}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-table/flink-table-planner/README.md', 'node_id': 4371807}, 'preview': \"# Table Planner\\n\\nThis module connects Table/SQL API and runtime. It is responsible for translating and optimizing a table program into a Flink pipeline. \\nFor user documentation, check the [table documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/overview/).\\n\\nThis README contains some development info for table planner contributors.\\n\\n## Immutables for rules\\n\\nCalcite's `@ImmutableBeans.Property` has been removed in Calcite 1.28.0.\\n\\nSince Flink 1.17, planner rules use [Immutables](http://immutables.github.io/) to generate immutable\\nclasses for rule configs based on the provided interface. A config should be annotated with `@Value.Immutable`.\\nIn case the config is a nested class, the enclosing one should be annotated with `@Value.Enclosing`.\\n\\nOnce a new rule config is written and annotated, compile the module to generate the immutable class\\nfor that config. Generated code will be placed in `target/generated-sources/annotations`. The config\\ncan then be instantiated with the help of the generated immutable class like `Immutable<EnclosingClassName>.<ConfigClassName>`.\\n\\nIn case of issues, please double-check if a required generated class is present.\\nAs an example have a look at `org.apache.flink.table.planner.plan.rules.logical.EventTimeTemporalJoinRewriteRule`.\\nSee also `org.apache.calcite.plan.RelRule` for detailed explanation from Calcite.\\n\\n## Json Plan unit tests\\n\\nUnit tests verifying the JSON plan changes (e.g. Java tests in `org.apache.flink.table.planner.plan.nodes.exec.stream`) \\ncan regenerate all the files setting the environment variable `PLAN_TEST_FORCE_OVERWRITE=true`.\"}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-runtime-web/README.md', 'node_id': 4901010}, 'preview': '<!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\"License\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n-->\\n\\n# Apache Flink Web Dashboard\\n\\nThe web dashboard is work in progress towards the new Flink runtime monitor. In particular, it will\\nprovide the following missing features of the current web dashboard:\\n\\n - Live progress monitoring (via live accumulators)\\n - A graph view of the program, as it is executed.\\n - A REST style API to access the status of individual jobs.\\n - A more modular design\\n\\nThe dashboard listens at `http://localhost:8081`.\\n\\nThe new web dashboard is work in progress. It starts an HTTP server (by default at port 8081)\\nthat serves the new web pages and additional background requests.\\n\\n## Server Backend\\n\\nThe server side of the dashboard is implemented using [Netty](http://netty.io) with\\n[Netty Router](https://github.com/sinetja/netty-router) for REST paths.\\nThe framework has very lightweight dependencies.\\n\\nThe code is regular Java code built via Maven. To add additional request handlers, follow the\\nexample of the `org.apache.flink.runtime.webmonitor.handlers.JobSummaryHandler`.\\n\\n\\n## Dashboard Frontend \\n\\nThe web dashboard is implemented using *Angular*. The dashboard build infrastructure uses *node.js*.\\n\\n\\n### Preparing the Build Environment\\n\\nDepending on your version of Linux, Windows or MacOS, you may need to manually install *node.js*\\n\\n\\n\\n#### Ubuntu Linux\\n\\nInstall *node.js* by following [these instructions](https://nodejs.org/en/download/).\\n\\nVerify that the installed version is at least *18.19.1*, via `node --version`.\\n\\n\\n#### MacOS\\n\\nFirst install *brew* by following [these instructions](http://brew.sh/).\\n\\nInstall *node.js* via:\\n\\n```\\nbrew install node\\n```\\n\\n### Building\\n\\nThe build process downloads all requires libraries via the *node.js* package management tool (*npm*)\\nThe final build tool is *@angular/cli*.\\n\\n```\\ncd flink-runtime-web/web-dashboard\\nnpm install\\nnpm run build\\n```\\n\\nThe dashboard code is under `/src`. The result of the build process is under `/web`.\\n\\n### Developing\\n\\nWhen developing the dashboard, every change needs to recompile the files and update the server:\\n\\n```\\ncd flink-runtime-web/web-dashboard\\nnpm run build\\ncd ../../flink-dist\\nmvn -DskipTests clean package\\n```\\n\\nTo simplify continuous development, one can use a *standalone proxy server*, together with automatic\\nre-compilation:\\n\\n1. Start the proxy server via `npm run proxy` (You can modify the proxy target in the `proxy.conf.json`, the default proxy target is `localhost:8081`)\\n2. Access the dashboard at [`http://localhost:4200`](http://localhost:4200)\\n\\n### CodeStyle & Lint\\n\\n> Tips: Before committing, make sure you\\'ve run `npm run lint-staged` and that no errors are reported.\\n\\n```bash\\n$ npm run lint\\n```\\n\\n### How to update the NOTICE\\n> Tip: If you\\'ve made a modification to any of the dependencies, make sure you\\'ve run `dev/generate_notice` before committing to update the `NOTICE` file.\\n\\n```bash\\n./web-dashboard/dev/generate_notice.sh\\n```\\n\\n### Dependency\\n\\n- Framework: [Angular](https://angular.dev)\\n- CLI Tools: [Angular CLI](https://angular.dev/tools/cli)\\n- UI Components: [NG-ZORRO](https://github.com/NG-ZORRO/ng-zorro-antd)'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-python/README.md', 'node_id': 5617655}, 'preview': '# Apache Flink\\n\\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\\n\\nLearn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n\\n## Python Packaging\\n\\nPyFlink is a Python API for Apache Flink that allows you to build scalable batch and streaming workloads,\\nsuch as real-time data processing pipelines, large-scale exploratory data analysis, Machine Learning (ML)\\npipelines and ETL processes. If you’re already familiar with Python and libraries such as Pandas,\\nthen PyFlink makes it simpler to leverage the full capabilities of the Flink ecosystem.\\nDepending on the level of abstraction you need, there are two different APIs that can be used in PyFlink: PyFlink Table API and PyFlink DataStream API.\\n\\nThe PyFlink Table API allows you to write powerful relational queries in a way that is similar to\\nusing SQL or working with tabular data in Python. You can find more information about it via the tutorial\\n[https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/table_api_tutorial/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/table_api_tutorial/)\\n\\nThe PyFlink DataStream API gives you lower-level control over the core building blocks of Flink,\\nstate and time, to build more complex stream processing use cases.\\nTutorial can be found at [https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/datastream_tutorial/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/datastream_tutorial/)\\n\\nYou can find more information via the documentation at [https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/)\\n\\nThe auto-generated Python docs can be found at [https://nightlies.apache.org/flink/flink-docs-stable/api/python/](https://nightlies.apache.org/flink/flink-docs-stable/api/python/)\\n\\n## Python Requirements\\n\\nApache Flink Python API depends on Py4J (currently version 0.10.9.7), CloudPickle (currently version 2.2.0), python-dateutil (currently version >=2.8.0,<3), Apache Beam (currently version >= 2.54.0, <= 2.61.0).\\n\\n## Development Notices\\n\\n### Protobuf Code Generation\\n\\nProtocol buffer is used in file `flink_fn_execution_pb2.py` and `flink_fn_execution_pb2.pyi`, the file is generated from `flink-fn-execution.proto`. Whenever `flink-fn-execution.proto` is updated, please re-generate `flink_fn_execution_pb2.py` and `flink_fn_execution_pb2.pyi` by executing:\\n\\n```\\npython pyflink/gen_protos.py\\n```\\n\\nPyFlink depends on the following libraries to execute the above script:\\n1. grpcio-tools (>=1.29.0,<=1.71.0)\\n2. setuptools (>=37.0.0)\\n3. pip (>=20.3)\\n\\n### Running Test Cases\\n\\nCurrently, we use tox and to verify the compatibility of the Flink Python API for multiple versions of Python and will integrate some useful plugins with tox, such as flake8.\\nWe can enter the directory where this README.md file is located and run test cases by executing\\n\\n```\\n./dev/lint-python.sh\\n```\\n\\nTo use your system uv environment, you can set `FLINK_UV_HOME` variable:\\n\\n```shell\\nexport FLINK_UV_HOME=$(dirname $(dirname $(which uv)))\\n```\\n\\nCreate a virtual environment:\\n```shell\\nuv venv pyflink_39 --python=3.9\\n```\\n\\nThen you can activate your environment and run tests, for example:\\n\\n```shell\\nsource pyflink_39/bin/activate\\nuv pip install --group dev\\n./dev/lint-python.sh\\n```'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-python/apache-flink-libraries/README.md', 'node_id': 5709492}, 'preview': '# Apache Flink\\n\\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\\n\\nLearn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n\\n## Python Packaging\\n\\nThis package bundles [apache-flink](https://pypi.org/project/apache-flink/) needed libraries'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/README.md', 'node_id': 5867943}, 'preview': \"Example: Running a state machine for pattern detection\\n======================================================\\n\\nThis example illustrates a minimal roll-your-own event pattern detection scenario,\\nusing a simple state machine that is evaluated over the stream.\\n\\nWhile this example is much simpler and more manual than what the CEP library supports,\\nit illustrates the use of event processing and state management for a medium\\ncomplex scenario.\\n\\n**Scenario Description**\\n\\nEvents in streams are expected to occur in certain patterns. Any deviation from\\nthese patterns indicates an anomaly that the streaming system should recognize and that\\nshould trigger an alert.\\n\\nYou can, for example, think of events as being generated by network devices and services,\\nsuch as firewalls login-, and registration with an authentication service, etc. A deviation\\nfrom the expected pattern might indicate an intrusion detection.\\n\\nThe event patterns are tracked per interacting party (here simplified per source IP address)\\nand are validated by a state machine. The state machine's states define what possible\\nevents may occur next, and what new states these events will result in.\\n\\nThe following diagram depicts the state machine used in this example.\\n\\n```\\n           +--<a>--> W --<b>--> Y --<e>---+\\n           |                    ^         |\\n   INITIAL-+                    |         |\\n           |                    |         +--> (Z) -----<g>---> TERM\\n           +--<c>--> X --<b>----+         |\\n                     |                    |\\n                     +--------<d>---------+\\n```\\n\\n**Example Program**\\n\\nThe main class of this example program is `org.apache.flink.streaming.examples.statemachine.StateMachineExample`.\\nThe core logic is in the `flatMap` function, which runs the state machines per IP address.\\n\\nThe streaming data flow is as shown below, where the source stream may come from either\\nan embedded data generator, or from a from a Kafka topic:\\n\\n```\\n [ stream partition 1] --> source --> partition -+---> flatMap(state machine) --> sink\\n                                            \\\\/\\n                                            /\\\\\\n [ stream partition 2] --> source --> partition -+---> flatMap(state machine) --> sink\\n```\"}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/README.md', 'node_id': 5871276}, 'preview': \"# Flink End-to-End Tests\\n\\nThis module contains tests that verify end-to-end behaviour of Flink. \\n\\nThe tests defined in `run-nightly-tests.sh` are run by the CI system on every pull request\\nand push to master.\\n\\n\\n## Running Tests\\nYou can run all tests by executing\\n\\n```\\n$ FLINK_DIR=<flink dir> flink-end-to-end-tests/run-nightly-tests.sh\\n```\\n\\nwhere \\\\<flink dir\\\\> is a Flink distribution directory.\\n\\nYou can also run tests individually via\\n\\n```\\n$ FLINK_DIR=<flink dir> flink-end-to-end-tests/run-single-test.sh your_test.sh arg1 arg2\\n```\\n\\n**NOTICE**: Please _DON'T_ run the scripts with explicit command like ```sh run-nightly-tests.sh``` since ```#!/usr/bin/env bash``` is specified as the header of the scripts to assure flexibility on different systems.\\n\\n**NOTICE**: We do not recommend executing the nightly test script on production or personal desktop systems, as tests contained there might modify the environment (leftover processes, modification of system files, request for root permissions via sudo, ...).\\n\\n### Kubernetes test\\n\\nKubernetes test (test_kubernetes_embedded_job.sh) assumes a running minikube cluster.\\n\\n\\n## Writing Tests\\n\\nAs of November 2020, Flink has two broad types of end-to-end tests: Bash-based end-to-end tests, located in the `test-scripts/` directory and Java-based end-to-end tests, such as the `PrometheusReporterEndToEndITCase`. The community recommends writing new tests as Java tests, as we are planning to deprecate the bash-based tests in the long run.\\n\\n\\n### Examples\\nHave a look at `test_batch_wordcount.sh` for a very basic test and\\n`test_streaming_kafka010.sh` for a more involved example. Whenever possible, try\\nto put new functionality in `common.sh` so that it can be reused by other tests.\\n\\n### Adding a test case\\nIn order to add a new test case you need add it to `test-scripts/run-nightly-tests.sh`. Templates on how to add tests can be found in those respective files.\\n\\n_Note: If you want to parameterize your tests please do so by adding multiple test cases with parameters as arguments to the nightly / pre-commit test suites. This allows the test runner to do a cleanup in between each individual test and also to fail those tests individually._\\n\\n_Note: While developing a new test case make sure to enable bash's error handling in `test-scripts/common.sh` by uncommenting `set -Eexuo pipefail` and commenting the current default `set` call. Once your test is implemented properly, add `set -Eeuo pipefail` on the very top of your test script (before any `common` script)._\\n\\n### Passing your test\\nA test is considered to have passed if it:\\n- has exit code 0\\n- there are no non-empty .out files (nothing was written to stdout / stderr by your Flink program)\\n- there are no exceptions in the log files\\n- there are no errors in the log files\\n\\n_Note: There is a whitelist for exceptions and errors that do not lead to failure, which can be found in the `check_logs_for_errors` and `check_logs_for_exceptions` in `test-scripts/common.sh`._\\n\\nPlease note that a previously supported pattern where you could assign a value the global variable `PASS` to have your tests fail **is not supported anymore**.\\n\\n### Cleanup\\nThe test runner performs a cleanup after each test case, which includes:\\n- Stopping the cluster\\n- Killing all task and job managers\\n- Reverting `conf` and `lib` dirs to default\\n- Cleaning up log and temp directories\\n\\nIn some cases your test is required to do some *additional* cleanup, for example shutting down external systems like Kafka or Elasticsearch. In this case you can register a function that will be called on test exit like this:\\n\\n```sh\\nfunction test_cleanup {\\n    # do your custom cleanup here\\n}\\n\\non_exit test_cleanup\\n```\"}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/README.md', 'node_id': 5881602}, 'preview': '# Apache Hadoop Docker image with Kerberos enabled\\n\\nRequired versions\\n-----------------\\n\\n* JDK8\\n* Hadoop 2.10.2\\n\\nDefault Environment Variables\\n-----------------------------\\n\\n| Name                      | Value | Description                                                                                                                                     |\\n|---------------------------| ----  |-------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `KRB_REALM`               | `EXAMPLE.COM` | The Kerberos Realm, more information [here](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html#)                        |\\n| `DOMAIN_REALM`            | `example.com` | The Kerberos Domain Realm, more information [here](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html#)                 |\\n| `KERB_MASTER_KEY`         | `masterkey` | The Kerberos master database password, more information [here](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/admin_commands/kdb5_util.html#) |\\n| `KERBEROS_ADMIN_USER`     | `admin/admin` | The KDC admin user                                                                                                                              |\\n| `KERBEROS_ADMIN_PASSWORD` | `admin` | The KDC admin password                                                                                                                          |\\n\\nYou can simply define these variables in the `docker-compose.yml`.\\n\\nRun image\\n---------\\n\\n```\\ncd flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster\\nwget -O hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-2.10.2/hadoop-2.10.2.tar.gz\\ndocker compose build\\ndocker compose up\\n```\\n\\nUsage\\n-----\\n\\nGet the container name with `docker ps` and login to the container with\\n\\n```\\ndocker exec -it <container-name> /bin/bash\\n```\\n\\nTo obtain a Kerberos ticket, execute\\n\\n```\\nkinit -kt /home/hadoop-user/hadoop-user.keytab hadoop-user\\n```\\n\\nAfterwards you can use `hdfs` CLI like\\n\\n```\\nhdfs dfs -ls /\\n```\\n\\nKnown issues\\n------------\\n\\n### Unable to obtain Kerberos password\\n\\n#### Error\\ndocker compose up fails for the first time with the error\\n\\n```\\nLogin failure for nn/hadoop.docker.com@EXAMPLE.COM from keytab /etc/security/keytabs/nn.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user\\n```\\n\\n#### Solution\\n\\nStop the containers with `docker compose down` and start again with `docker compose up -d`.\\n\\n### Java Keystore\\n\\nIf the Keystore has been expired, then create a new `keystore.jks`:\\n\\n1. create private key\\n\\n```\\nopenssl genrsa -des3 -out server.key 1024\\n```\\n\\n2. create csr\\n\\n```\\nopenssl req -new -key server.key -out server.csr`\\n```\\n\\n3. remove passphrase in key\\n```\\ncp server.key server.key.org\\nopenssl rsa -in server.key.org -out server.key\\n```\\n\\n3. create self-signed cert\\n```\\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\\n```\\n\\n4. create JKS and import certificate\\n```\\nkeytool -import -keystore keystore.jks -alias CARoot -file server.crt`\\n```'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/README.md', 'node_id': 5885771}, 'preview': '# Flink [TPC-DS]( http://www.tpc.org/tpcds) test\\n\\nYou can run the Flink TPC-DS test in your local environment as following commands:\\n\\n```\\n$ cd <your_workspace_dir>/flink                                                                               # go to the Flink project   \\n$ mvn compile -DskipTests=true                                                                               # compile Flink source code\\n$ export FLINK_DIR=<your_workspace_dir>/flink/flink-dist/target/flink-${flink-version}-bin/flink-${flink-version}  # set Flink distribution directory, the ${flink-version} is the compiled Flink version\\n$ sh flink-end-to-end-tests/run-single-test.sh flink-end-to-end-tests/test-scripts/test_tpcds.sh               # run a TPC-DS benchmark\\n```\\n\\nThe default scale factor of TPC-DS benchmark in Flink is 1GB, Flink use this scale factor from validation purpose.'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-end-to-end-tests/flink-tpcds-test/tpcds-tool/README.md', 'node_id': 5885774}, 'preview': \"# TPC-DS tool\\n\\nThis directory contains the necessary tools to run a [TPC-DS]( http://www.tpc.org/tpcds) benchmark.\\n\\n## data_generator.sh scripts\\n\\nScripts to download the TPC-DS data generator and generate test data, the supported OS including GNU/Linux OS and Mac OS.\\n```\\n data_generator.sh <generatorDir> <scaleFactor> <outputDataDir> <commonScriptsDir>\\n```\\n* generator_dir: directory to save TPC-DS data generator.\\n* scale_factor: scale factor indicates raw data size in GB, we use the validation scale factor 1 (1G) by default.\\n* data_dir: directory to save TPC-DS test data.\\n* common_scripts_dir: directory which contains some common scripts for Flink end to end tests, the retry scripts `retry_times_with_backoff_and_cleanup()` will be used when download the TPC-DS data generator from the Internet.\\n\\nThe scripts is internally used in Flink end to end tests, give an example to specific how the scripts works:\\n```\\n$ cd <your_workspace_dir>/flink                                                                                   # go to the Flink project\\n$ mvn compile -DskipTests=true                                                                                   # compile the Flink source code\\n$ export FLINK_DIR=<your_workspace_dir>/flink/flink-dist/target/flink-${flink-version}-bin/flink-${flink-version}      # set Flink distribution directory, the ${flink-version} is the compiled Flink version\\n$ export END_TO_END_DIR=<your_workspace_dir>/flink/flink-end-to-end-tests                                          # set end to end tests directory\\n$ mkdir -p <your_workspace_dir>/dir_to_save_genarator\\n$ mkdir -p <your_workspace_dir>/dir_to_save_data\\n$ cd <your_workspace_dir>/flink/flink-end-to-end-tests/flink-tpcds-test/tpcds-tool\\n$ sh data_generator.sh <your_workspace_dir>/dir_to_save_genarator 1 <your_workspace_dir>/dir_to_save_data <your_workspace_dir>/flink/flink-end-to-end-tests/test-scripts\\n```\\nThe downloaded generator will be saved to `<your_workspace_dir>/dir_to_save_genarator`, the generated data will be saved to `<your_workspace_dir>/dir_to_save_data`.\\n\\n**NOTE:** If you want to run a TPC-DS benchmark in Flink, please read `flink-tpcds-test/README.md` for more information.\\n    \\n## query directory\\n\\nDirectory that contains all TPC-DS queries (103 queries in total), the corresponding data scale factor is 1G, these queries come from TPC-DS standard specification v2.11.0 [1].\\n\\n**NOTE:** The scale factor of queries should match with test data's scale factor.\\n    \\n## answer_set directory\\n\\nDirectory that contains all answers for per query, the answer content of query may has different order according to the system's null order.\\n\\n* (1) Flink SQL keeps null first in ASC order, keeps null last in DESC order, we choose corresponding answer file when compare the answer.\\n* (2) For query 8, 14a, 18, 70 and 77, the decimal precision of answer set is too low, we compare query result with SQL server's answer, they can strictly match.\\n   \\n ## Reference   \\n[1] http://www.tpc.org/tpc_documents_current_versions/pdf/tpc-ds_v2.11.0.pdf\"}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-docs/README.md', 'node_id': 5910092}, 'preview': '<!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\"License\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n-->\\n\\n# Documentation generators\\n\\nThis module contains generators that create HTML files directly from Flink\\'s source code.\\n\\n## REST API documentation\\n\\nThe `RestAPIDocGenerator`/`OpenApiSpecGenerator` can be used to generate a full reference of the REST API of a `RestServerEndpoint`. A separate file is generated for each endpoint.\\n\\nTo integrate a new endpoint into the generator\\n1. Add a new `DocumentingRestEndpoint` class to `RestAPIDocGenerator` that extends the new endpoint class\\n2. Add another call to `createHtmlFile` in `RestAPIDocGenerator#main`\\n3. Regenerate the documentation by running `mvn package -Dgenerate-rest-docs -pl flink-docs -am -nsu -DskipTests`\\n4. Integrate the generated file into the REST API documentation by adding `{{< generated/<file-name> }}` to the corresponding markdown file.\\n\\nThe documentation must be regenerated whenever\\n* a handler is added to/removed from a `RestServerEndpoint`\\n* any used `MessageHeaders` class or any referenced `RequestBody`, `ResponseBody`, `MessageParameters` or `MessageParameter` class is modified.\\n\\n## Configuration documentation\\n\\nThe `ConfigOptionsDocGenerator` can be used to generate a reference of `ConfigOptions`. By default, a separate file is generated for each `*Options` class found in `org.apache.flink.configuration` and `org.apache.flink.yarn.configuration`. The `@ConfigGroups` annotation can be used to generate multiple files from a single class.\\n\\nTo integrate an `*Options` class from another package, add another module-package argument pair to `ConfigurationOptionLocator#LOCATIONS`.\\n\\nThe files can be generated by running `mvn package -Dgenerate-config-docs -pl flink-docs -am -nsu -DskipTests -Pskip-webui-build`, and can be integrated into the documentation using `{{ include generated/<file-name> >}}`.\\n\\nThe documentation must be regenerated whenever\\n* an `*Options` class was added or removed\\n* a `ConfigOption` was added to or removed from an `*Options` class\\n* a `ConfigOption` was modified in any way.'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/README.md', 'node_id': 6149611}, 'preview': '# flink-architecture-tests\\n\\nThis module contains architecture tests using [ArchUnit](https://www.archunit.org/). Considering the\\nisolation of classpath and rules for architectural tests, there are two top level categories:\\n\\n- production code architectural tests\\n- test code architectural tests\\n\\nSince both of them will need some common ArchUnit extensions, there are three submodules:\\n\\n- flink-architecture-tests-base - contains common ArchUnit extensions that will be used for both\\n  production code architectural tests and test code architectural tests.\\n- flink-architecture-tests-production - contains all architectural rules and tests centrally for\\n  production code. Please read the [README](flink-architecture-tests-production/README.md) of\\n  flink-architecture-tests-production for further information.\\n- flink-architecture-tests - contains architectural rules centrally for test code. The architectural\\n  test will be built individually in each submodule where the test code has been developed. Please\\n  read the [README](flink-architecture-tests-test/README.md) of flink-architecture-tests-test for\\n  further information.\\n\\nFollowing documentation is valid for building and maintaining the architectural tests both for the\\nproduction code and the test code.\\n\\n## What should I do if the tests fail?\\n\\nThere are two cases in which architectural tests may fail due to changes in the codebase:\\n\\n1. You have resolved an existing violation.\\n2. Your change introduced a new violation.\\n\\nIn the former case, please add the updated violation store file to your commit. The tests should now\\nsucceed.\\n\\nIf you have added a new violation, consider how to proceed. New violations should be avoided at all\\ncosts. First and foremost, evaluate whether the flagged violation is correct. If it is, try to\\nrework your change to avoid the violation in the first place. However, if you believe that your code\\nshould not be flagged as a violation, open a JIRA issue so that the rule can be improved.\\n\\nIn order to have a new violation recorded, open `archunit.properties` in the submodule and enable\\n`freeze.refreeze=true`. Rerun the tests and revert the change to the configuration. The new\\nviolation should now have been added to the existing store.\\n\\n## How do I write a new architectural rule?\\n\\nPlease refer to the [ArchUnit user guide](https://www.archunit.org/) for general documentation.\\nHowever, there are a few points to consider when writing rules:\\n\\n1. If there are existing violations which cannot be fixed right away, the rule must be _frozen_ by\\n   wrapping it in `FreezingArchRule.freeze()`. This will add the rule to the violation store that\\n   records the existing violations. Add the new stored violations file within `violations` to your\\n   commit.\\n2. Please give the freezing rule a fixed description by\\n   calling `FreezingArchRule.freeze().as(String newDescription)`. This will reduce the maintenance\\n   effort for each rule update. Otherwise, since the description will be used as the key of the rule\\n   to define the violation store, new violation stores will be created and old obsolete stores have\\n   to be removed manually each time when rules have been changed.\\n3. In order to allow creating new violation store file, open `archunit.properties` in the submodule\\n   and enable `freeze.store.default.allowStoreCreation=true`.\\n4. ArchUnit does not work well with Scala classes. All rules should exclude non-Java classes by\\n   utilizing the methods in `GivenJavaClasses`.\\n\\n## How do I test Scala classes?\\n\\nScala is not supported by ArchUnit. Although it operates on the byte-code level and can, in general,\\nprocess classes compiled from Scala as well, there are Scala-specific constructs that do not work\\nwell in practice. Therefore, all architecture rules should exclude non-Java classes.'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-test/README.md', 'node_id': 6149615}, 'preview': '# flink-architecture-tests-test\\n\\nThis submodule contains rules defined centrally for test code architectural tests that will be\\ndeveloped in each submodule individually where the test code has been developed.\\n\\n## Why the architecture test infra for production code and for test code are different\\n\\nCompared to `flink-architecture-tests-production` which implements and executes architectural rules\\ncentrally, `flink-architecture-tests-test` implements the rules centrally, but they are executed\\nindividually in each submodule. This is done for the following reasons:\\n\\n- Reduce the classpath complexity. In order to execute the tests centrally, each submodule would\\n  have to provide a test-jar. However, this does not work well in IntelliJ which doesn\\'t support all\\n  Maven configurations (such as exclusion filters); this would cause the rules to behave differently\\n  when executed from Maven versus IntelliJ. Furthermore, creating more test-jars is not desirable\\n  and increases the project complexity.\\n- Separation of concerns. Test code should be viewed as internal to a module, and by separating the\\n  violation stores for each submodule this separation can be maintained. Unlike the production code\\n  tests there is few shared code between test code, and thus the performance benefits do not apply.\\n- Flexibility. Each submodule can not only import the generic rules defined\\n  in `flink-architecture-tests-test` centrally, but also develop further module-specific\\n  architectural tests locally.\\n\\n## How do I write a new architectural rule?\\n\\nPlease refer to [README](../README.md).\\n\\n## How do I initialize and develop the first architectural test for a Flink submodule\\'s test code?\\n\\nIf there already exists any architectural test in the Flink submodule, you can skip this section and\\nstart writing your architectural tests and rules.\\n\\nIt is recommended to stick to the following template to setup and develop the first test code\\narchitectural test for a Flink submodule.\\n\\nUnder a Flink submodule where no architectural test has ever been developed.\\n\\n- Create `archunit.properties` and `flink-connector-file` under \"<Flink Submodule>\\n  /src/test/resources\". Please use [archunit.properties](./src/test/resources/archunit.properties)\\n  and [log4j2-test.properties](./src/test/resources/log4j2-test.properties) as template.\\n- Develop ArchUnit test under the package `org.apache.flink.architecture`. It is recommended to\\n  use `TestCodeArchitectureTest` as the class name.\\n- Include the common tests\\n  via `@ArchTest public static final ArchTests COMMON_TESTS = ArchTests.in(TestCodeArchitectureTestBase.class)`\\n  .\\n- Develop individual rules under the package `org.apache.flink.architecture.rules` if it is\\n  required.\\n\\nFor practice purpose, please refer to the `flink-connector-file` as the reference implementation.\\n\\n## What should I do if the tests fail?\\n\\nPlease refer to [README](../README.md).\\n\\n## How do I test Scala classes?\\n\\nPlease refer to [README](../README.md).\\n\\n## What should I do if I modify an existing rule?\\n\\nIn that case, you need to regenerate all the stores. Just run this on the root of the project:\\n\\n```\\nrm -rf `find . -type d -name archunit-violations`\\nmvn test -Dtest=\"*TestCodeArchitectureTest*\" -DfailIfNoTests=false -Darchunit.freeze.refreeze=true -Darchunit.freeze.store.default.allowStoreCreation=true -Dfast\\n```'}, {'FileNode': {'basename': 'README.md', 'relative_path': 'flink-architecture-tests/flink-architecture-tests-production/README.md', 'node_id': 6149940}, 'preview': \"# flink-architecture-tests-production\\n\\nThis submodule contains all architectural tests of production code. \\nRunning these tests together (rather than individually per module) allows caching the imported\\nclasses for better performance.\\n\\n## What should I do if the tests fail?\\n\\nPlease refer to [README](../README.md).\\n\\n## How do I write a new architectural rule?\\n\\nPlease refer to [README](../README.md).\\n\\n## How do I write a ArchUnit test?\\n\\nPlease refer to the [ArchUnit user guide](https://www.archunit.org/) for general documentation. For\\nquick start, you could find an example\\nat [flink-architecture-tests-production/ArchitectureTest](./src/test/java/org/apache/flink/architecture/ArchitectureTest.java):\\n\\n```java\\n@ArchTest\\npublic static final ArchTests API_ANNOTATIONS=ArchTests.in(ApiAnnotationRules.class);\\n```\\n\\n## How do I add a module?\\n\\nIn order to add a module to be tested against, add it as a test dependency in this module's\\n`pom.xml`.\\n\\n## How do I test Scala classes?\\n\\nPlease refer to [README](../README.md).\"}, {'FileNode': {'basename': 'README.md', 'relative_path': 'docs/README.md', 'node_id': 6152186}, 'preview': 'This README gives an overview of how to build and contribute to the documentation of Apache Flink.\\n\\nThe documentation is included with the source of Apache Flink in order to ensure that you always\\nhave docs corresponding to your checked out version. The online documentation at\\nhttps://flink.apache.org/ is also generated from the files found here.\\n\\n# Requirements\\n\\n### Build the documentation and serve it locally\\n\\nThe Flink documentation uses [Hugo](https://gohugo.io/getting-started/installing/) to generate HTML files.  More specifically, it uses the *extended version* of Hugo with Sass/SCSS support. \\n\\nTo build the documentation, you can install Hugo locally or use a Docker image. \\n\\nBoth methods require you to execute commands in the directory of this module (`docs/`). The built site is served at http://localhost:1313/.\\n\\n#### Using Hugo Docker image:\\n\\n```sh\\n$ git submodule update --init --recursive\\n$ ./setup_docs.sh\\n$ docker pull jakejarvis/hugo-extended:latest\\n$ docker run -v $(pwd):/src -p 1313:1313 jakejarvis/hugo-extended:latest server --buildDrafts --buildFuture --bind 0.0.0.0\\n```\\n\\n#### Local Hugo installation:\\n\\nMake sure you have installed [Hugo](https://gohugo.io/getting-started/installing/) on your system.\\n\\n```sh\\n$ ./setup_hugo.sh\\n```\\n\\nThen build the docs from source:\\n\\n```sh\\n$ ./build_docs.sh\\n```\\n\\nThe shell `./build_docs.sh` will integrate external connector docs, referencing `setup_docs.sh#integrate_connector_docs`.\\nThis process involves cloning the repo of some external connectors, which can be time-consuming and prone to network issues.\\nSo, if the connector docs have been synced before, and you wish to skip this step, you can do so by adding the following arg:\\n\\n```sh\\n$ ./build_docs.sh --skip-integrate-connector-docs\\n```\\n\\nThe site can be viewed at http://localhost:1313/\\n\\n## Include externally hosted documentation\\n\\nWith the ongoing efforts to move Flink\\'s connectors from this repository to individual, dedicated\\nrepositories, this also requires the documentation to be hosted outside this repo. However, \\nwe still want to serve all documentation as a whole on the Flink documentation website.\\n\\nAdding new externally hosted documentation requires the following steps to be taken:\\n\\n1. (If necessary) Move the existing documentation to the new repository\\n\\n2. In the Flink repository, edit the `docs/setup_docs.sh` file and add a reference to your now \\nexternally hosted documentation. The reference will look like `integrate_connector_docs <connector_name> <branch_or_tag>`.\\n\\nReplace <connector_name> with the name of your connector, e.g., `elasticsearch` for `flink-connector-elasticsearch`.\\n\\n## Generate configuration tables\\n\\nConfiguration descriptions are auto generated from code. To trigger the generation, you need to run a command in the project root (see [Configuration documentation](https://github.com/apache/flink/blob/master/flink-docs/README.md#configuration-documentation).)\\n\\nThe resulting html files will be written to `layouts/shortcodes/generated`.\\n\\n# Contribute\\n\\n## Markdown\\n\\nThe documentation pages are written in [Markdown](http://daringfireball.net/projects/markdown/syntax). It is possible to use [GitHub flavored syntax](http://github.github.com/github-flavored-markdown) and intermix plain html.\\n\\n## Front matter\\n\\nIn addition to Markdown, every page contains a Jekyll front matter, which specifies the title of the page and the layout to use. The title is used as the top-level heading for the page. The default layout is `plain` (found in `_layouts`).\\n\\n    ---\\n    title: \"Title of the Page\"\\n    ---\\n    \\n    ---\\n    title: \"Title of the Page\" <-- Title rendered in the side nave\\n    weight: 1 <-- Weight controls the ordering of pages in the side nav. \\n    type: docs <-- required\\n    aliases:  <-- Alias to setup redirect from removed page to this one\\n      - /alias/to/removed/page.html\\n    ---\\n\\n## Structure\\n\\n### Page\\n\\n#### Headings\\n\\nAll documents are structured with headings. From these headings, you can automatically generate a page table of contents (see below).\\n\\n```\\n# Level-1 Heading  <- Used for the title of the page \\n## Level-2 Heading <- Start with this one for content\\n### Level-3 heading\\n#### Level-4 heading\\n##### Level-5 heading\\n```\\n\\nPlease stick to the \"logical order\" when using the headlines, e.g. start with level-2 headings and use level-3 headings for subsections, etc. Don\\'t use a different ordering, because you don\\'t like how a headline looks.\\n\\n#### Table of Contents\\n\\nTable of contents are added automatically to every page, based on heading levels 2 - 4. \\nThe ToC can be omitted by adding the following to the front matter of the page:\\n\\n    ---\\n    bookToc: false\\n    ---\\n\\n### ShortCodes \\n\\nFlink uses [shortcodes](https://gohugo.io/content-management/shortcodes/) to add custom functionality\\nto its documentation markdown. The following are available for use:  \\n\\n#### Flink Artifact\\n\\n    {{< artifact flink-table-api-scala withScalaVersion >}}\\n\\nThis will be replaced by the maven artifact for flink-table-api-scala that users should copy into their pom.xml file. It will render out to:\\n\\n```xml\\n<dependency>\\n    <groupId>org.apache.flink</groupId>\\n    <artifactId>flink-table-api-scala_2.12</artifactId>\\n    <version><!-- current flink version --></version>\\n</dependency>\\n```\\n\\nIt includes a number of optional flags:\\n\\n* withScalaVersion: Includes the scala version suffix to the artifact id\\n* withTestScope: Includes `<scope>test</scope>` to the module. Useful for marking test dependencies.\\n* withTestClassifier: Includes `<classifier>tests</classifier>`. Useful when users should be pulling in Flink tests dependencies. This is mostly for the test harnesses and probably not what you want. \\n\\nYou can also use the shortcodes (with same flags) instead:\\n\\n* `artifact_gradle` to show the Gradle syntax\\n* `artifact_tabs` to create a tabbed view, showing both Maven and Gradle syntax\\n\\n#### Flink Connector Artifact\\n\\n    {{< connector_artifact flink-connector-elasticsearch 3.0.0 >}}\\n\\nThis will be replaced by the maven artifact for flink-connector-elasticsearch that users should copy into their pom.xml file. It will render out to:\\n\\n```xml\\n<dependency>\\n    <groupId>org.apache.flink</groupId>\\n    <artifactId>flink-connector-elasticsearch</artifactId>\\n    <version>3.0.0</version>\\n</dependency>\\n```\\n\\n#### Back to Top\\n\\n\\t{{< top >}}\\n\\nThis will be replaced by a back to top link. It is recommended to use these links at least at the end of each level-2 section.\\n\\n#### Info Hints\\n\\n\\t{{< hint info >}}\\n\\tSome interesting information\\n\\t{{< /hint >}}\\n\\nThe hint will be rendered in a blue box. This hint is useful when providing \\nadditional information for the user that does not fit into the flow of the documentation.\\n\\n#### Info Warning \\n\\n    {{< hint warning >}}\\n    Something to watch out for. \\n    {{< /hint >}}\\n\\nThe hint will be rendered in a yellow box. This hint is useful when highlighting\\ninformation users should watch out for to prevent errors. \\n\\n#### Info Danger\\n\\n    {{< hint danger >}}\\n    Something to avoid\\n    {{< /hint >}}\\n\\nThe hint will be rendered in a red box. This hint is useful when highlighting\\ninformation users need to know to avoid data loss or to point out broken\\nfunctionality. \\n\\n#### Label\\n\\n    {{< label \"My Label\" >}}\\n\\nThe label will be rendered in an inlined blue box. This is useful for labeling functionality\\nsuch as whether a SQL feature works for only batch or streaming execution. \\n\\n#### Flink version \\n\\n    {{< version >}}\\n\\nInterpolates the current Flink version\\n\\n#### Scala Version\\n\\n    {{< scala_version >}}\\n\\nInterpolates the default scala version\\n\\n#### Stable\\n\\n    {{< stable >}}\\n     Some content\\n    {{< /stable >}}\\n\\nThis shortcode will only render its content if the site is marked as stable. \\n\\n#### Unstable \\n\\n    {{< unstable >}}\\n    Some content \\n    {{< /unstable >}}\\n\\nThis shortcode will only render its content if the site is marked as unstable. \\n\\n#### Query State Warning\\n\\n    {{< query_state_warning >}}\\n\\nWill render a warning the current SQL feature may have unbounded state requirements.\\n\\n#### tab\\n\\n    {{< tabs \"sometab\" >}}\\n    {{< tab \"Java\" >}}\\n    ```java\\n    System.out.println(\"Hello World!\");\\n    ```\\n    {{< /tab >}}\\n    {{< tab \"Scala\" >}}\\n    ```scala\\n    println(\"Hello World!\");\\n    ```\\n    {< /tab >}}\\n    {{< /tabs }}\\n\\nPrints the content in tabs. IMPORTANT: The label in the outermost \"tabs\" shortcode must\\nbe unique for the page. \\n\\n#### Github Repo\\n\\n    {{< github_repo >}}\\n\\nRenders a link to the apache flink repo. \\n\\n#### Github Link\\n\\n    {{< gh_link file=\"/some/file.java\" name=\"Some file\" >}}\\n\\nRenders a link to a file in the Apache Flink repo with a given name. \\n\\n#### JavaDocs Link\\n    {{< javadoc file=\"some/file\" name=\"Some file\" >}}\\n\\nRenders a link to a file in the Apache Flink Java Documentation. \\n\\n#### PythonDocs Link\\n    {< pythondoc file=\"some/file\" name=\"Some file\" >}}\\n\\nRenders a link to a file in the Apache Flink Python Documentation. \\n\\n#### FlinkDownloads Link\\n\\n```\\n{{< downloads >}}\\n```\\n\\nRenders a link to the apache flink download page.'}]",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "HumanMessage",
                    "content": "content='Check README.md, Quickstart, Installation, or Getting Started sections for a quick verification command.' additional_kwargs={} response_metadata={} id='9cfc84ce-bd88-4477-ba60-5756f63f37c2'",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                }
            ],
            "testsuite_refined_query": "Check README.md, Quickstart, Installation, or Getting Started sections for a quick verification command.",
            "testsuite_command": [
                "mvn --version",
                "java -version",
                "git --version",
                "find flink-yarn-tests/target -name \"*.err\" -or -name \"*.out\"",
                "node --version",
                "npm run lint",
                "python pyflink/gen_protos.py",
                "./dev/lint-python.sh",
                "docker --version",
                "docker compose version",
                "openssl version",
                "keytool -help",
                "mvn test -Dtest=\"*TestCodeArchitectureTest*\" -DfailIfNoTests=false -Darchunit.freeze.refreeze=true -Darchunit.freeze.store.default.allowStoreCreation=true -Dfast",
                "hugo version"
            ]
        },
        "env_states": {
            "max_refined_query_loop": 3,
            "env_implement_file_context_query": "OBJECTIVE: Find the most relevant environment configuration files that can help generate an accurate Dockerfile for this project,\nincluding Dockerfiles, dependency files, build configurations, and environment setup files.\n\n<reasoning>\n1. Analyze project characteristics:\n   - Programming language and framework\n   - Dependency management files\n   - Build system and tools\n   - Runtime requirements\n   - Environment configurations\n\n2. Search requirements:\n   - Existing Dockerfiles or containerization files\n   - Dependency files (requirements.txt, package.json, pom.xml, etc.)\n   - Build configuration files (Makefile, CMakeLists.txt, etc.)\n   - Environment configuration files (.env, config files)\n   - Setup and installation scripts\n\n3. Focus areas:\n   - Docker-related files (Dockerfile, docker-compose.yml, .dockerignore)\n   - Package management files (requirements.txt, package.json, go.mod, Cargo.toml, etc.)\n   - Build system files (Makefile, CMakeLists.txt, build.gradle, etc.)\n   - Environment configuration (.env, config.json, application.properties, etc.)\n   - Setup documentation (README.md, INSTALL.md, SETUP.md)\n   - CI/CD configuration files\n</reasoning>\n\nREQUIREMENTS:\n- Return the most relevant environment configuration files for Dockerfile generation\n- Must include complete file content with exact file paths and line numbers\n- Must include dependency files, build configurations, and environment setups\n- Must include any existing Docker-related files\n- Must include setup documentation and installation instructions\n\n<examples>\n<example id=\"python-web-app\">\n<project_context>\nPython Flask web application with requirements.txt\n</project_context>\n\n<ideal_files>\n# File: requirements.txt\nFlask==2.3.3\ngunicorn==21.2.0\npsycopg2-binary==2.9.7\nredis==4.6.0\n\n# File: app.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return 'Hello World!'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n\n# File: README.md\n## Installation\npip install -r requirements.txt\npython app.py\n</ideal_files>\n</example>\n\n<example id=\"nodejs-api\">\n<project_context>\nNode.js Express API with package.json\n</project_context>\n\n<ideal_files>\n# File: package.json\n{\n  \"name\": \"my-api\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"start\": \"node server.js\",\n    \"dev\": \"nodemon server.js\"\n  },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"mongoose\": \"^7.5.0\"\n  }\n}\n\n# File: server.js\nconst express = require('express');\nconst app = express();\nconst PORT = process.env.PORT || 3000;\n\napp.get('/', (req, res) => {\n  res.json({ message: 'API is running' });\n});\n\napp.listen(PORT, () => {\n  console.log(`Server running on port ${PORT}`);\n});\n</ideal_files>\n</example>\n\nSearch priority:\n1. Existing Dockerfiles or containerization files\n2. Dependency management files (requirements.txt, package.json, etc.)\n3. Build configuration files (Makefile, CMakeLists.txt, etc.)\n4. Environment configuration files (.env, config files)\n5. Setup documentation and installation instructions\n\nFind the most relevant environment configuration files with complete context for Dockerfile generation.\n",
            "env_implement_file_context": [
                {
                    "type": "Context",
                    "content": "File: flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/docker-compose.yml\nLine number range: 18 - 78\nContent:\n18. version: '3.5'\n19. \n20. networks:\n21.   docker-hadoop-cluster-network:\n22.     name: docker-hadoop-cluster-network\n23. \n24. services:\n25.   kdc:\n26.     build: kdc\n27.     container_name: \"kdc\"\n28.     hostname: kdc.kerberos.com\n29.     image: flink/test-kdc:latest\n30.     networks:\n31.       - docker-hadoop-cluster-network\n32.     environment:\n33.       REALM: EXAMPLE.COM\n34.       DOMAIN_REALM: kdc.kerberos.com\n35. \n36.   master:\n37.     build: hadoop\n38.     image: ${DOCKER_HADOOP_IMAGE_NAME:-flink/test-hadoop:latest}\n39.     command: master\n40.     depends_on:\n41.       - kdc\n42.     container_name: \"master\"\n43.     hostname: master.docker-hadoop-cluster-network\n44.     networks:\n45.       - docker-hadoop-cluster-network\n46.     environment:\n47.       KRB_REALM: EXAMPLE.COM\n48.       DOMAIN_REALM: kdc.kerberos.com\n49. \n50.   worker1:\n51.     build: hadoop\n52.     image: ${DOCKER_HADOOP_IMAGE_NAME:-flink/test-hadoop:latest}\n53.     command: worker\n54.     depends_on:\n55.       - kdc\n56.       - master\n57.     container_name: \"worker1\"\n58.     hostname: worker1.docker-hadoop-cluster-network\n59.     networks:\n60.       - docker-hadoop-cluster-network\n61.     environment:\n62.       KRB_REALM: EXAMPLE.COM\n63.       DOMAIN_REALM: kdc.kerberos.com\n64. \n65.   worker2:\n66.     build: hadoop\n67.     image: ${DOCKER_HADOOP_IMAGE_NAME:-flink/test-hadoop:latest}\n68.     command: worker\n69.     depends_on:\n70.       - kdc\n71.       - master\n72.     container_name: \"worker2\"\n73.     hostname: worker2.docker-hadoop-cluster-network\n74.     networks:\n75.       - docker-hadoop-cluster-network\n76.     environment:\n77.       KRB_REALM: EXAMPLE.COM\n78.       DOMAIN_REALM: kdc.kerberos.com\n",
                    "relative_path": "flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/docker-compose.yml",
                    "start_line_number": 18,
                    "end_line_number": 78
                },
                {
                    "type": "Context",
                    "content": "File: flink-python/setup.py\nLine number range: 31 - 34\nContent:\n31. if sys.version_info < (3, 9):\n32.     print(\"Python versions prior to 3.9 are not supported for PyFlink.\",\n33.           file=sys.stderr)\n34.     sys.exit(-1)\n",
                    "relative_path": "flink-python/setup.py",
                    "start_line_number": 31,
                    "end_line_number": 34
                },
                {
                    "type": "Context",
                    "content": "File: flink-python/setup.py\nLine number range: 319 - 330\nContent:\n319.     install_requires = ['py4j==0.10.9.7', 'python-dateutil>=2.8.0,<3',\n320.                         'apache-beam>=2.54.0,<=2.61.0',\n321.                         'cloudpickle>=2.2.0', 'avro>=1.12.0',\n322.                         'pytz>=2018.3', 'fastavro>=1.1.0,!=1.8.0', 'requests>=2.26.0',\n323.                         'protobuf>=3.19.0',\n324.                         'numpy>=1.22.4',\n325.                         'pandas>=1.3.0',\n326.                         'pyarrow>=5.0.0,<21.0.0',\n327.                         'pemja>=0.5.0,<0.5.4;platform_system != \"Windows\"',\n328.                         'httplib2>=0.19.0',\n329.                         'ruamel.yaml>=0.18.4',\n330.                         apache_flink_libraries_dependency]\n",
                    "relative_path": "flink-python/setup.py",
                    "start_line_number": 319,
                    "end_line_number": 330
                },
                {
                    "type": "Context",
                    "content": "File: flink-dist/src/main/resources/config.yaml\nLine number range: 19 - 24\nContent:\n19. # These parameters are required for Java 17 support.\n20. # They can be safely removed when using Java 8/11.\n21. env:\n22.   java:\n23.     opts:\n24.       all: --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED\n",
                    "relative_path": "flink-dist/src/main/resources/config.yaml",
                    "start_line_number": 19,
                    "end_line_number": 24
                },
                {
                    "type": "Context",
                    "content": "File: README.md\nLine number range: 1 - 10\nContent:\n1. # Apache Flink\n2. \n3. Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.\n4. \n5. Learn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\n6. \n7. \n8. ### Features\n9. \n10. * A streaming-first runtime that supports both batch processing and data streaming programs\n",
                    "relative_path": "README.md",
                    "start_line_number": 1,
                    "end_line_number": 10
                }
            ],
            "env_implement_write_messages": [
                {
                    "type": "HumanMessage",
                    "content": "content='Project Environment Context:\\nFile: flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/docker-compose.yml\\nLine number range: 18 - 78\\nContent:\\n18. version: \\'3.5\\'\\n19. \\n20. networks:\\n21.   docker-hadoop-cluster-network:\\n22.     name: docker-hadoop-cluster-network\\n23. \\n24. services:\\n25.   kdc:\\n26.     build: kdc\\n27.     container_name: \"kdc\"\\n28.     hostname: kdc.kerberos.com\\n29.     image: flink/test-kdc:latest\\n30.     networks:\\n31.       - docker-hadoop-cluster-network\\n32.     environment:\\n33.       REALM: EXAMPLE.COM\\n34.       DOMAIN_REALM: kdc.kerberos.com\\n35. \\n36.   master:\\n37.     build: hadoop\\n38.     image: ${DOCKER_HADOOP_IMAGE_NAME:-flink/test-hadoop:latest}\\n39.     command: master\\n40.     depends_on:\\n41.       - kdc\\n42.     container_name: \"master\"\\n43.     hostname: master.docker-hadoop-cluster-network\\n44.     networks:\\n45.       - docker-hadoop-cluster-network\\n46.     environment:\\n47.       KRB_REALM: EXAMPLE.COM\\n48.       DOMAIN_REALM: kdc.kerberos.com\\n49. \\n50.   worker1:\\n51.     build: hadoop\\n52.     image: ${DOCKER_HADOOP_IMAGE_NAME:-flink/test-hadoop:latest}\\n53.     command: worker\\n54.     depends_on:\\n55.       - kdc\\n56.       - master\\n57.     container_name: \"worker1\"\\n58.     hostname: worker1.docker-hadoop-cluster-network\\n59.     networks:\\n60.       - docker-hadoop-cluster-network\\n61.     environment:\\n62.       KRB_REALM: EXAMPLE.COM\\n63.       DOMAIN_REALM: kdc.kerberos.com\\n64. \\n65.   worker2:\\n66.     build: hadoop\\n67.     image: ${DOCKER_HADOOP_IMAGE_NAME:-flink/test-hadoop:latest}\\n68.     command: worker\\n69.     depends_on:\\n70.       - kdc\\n71.       - master\\n72.     container_name: \"worker2\"\\n73.     hostname: worker2.docker-hadoop-cluster-network\\n74.     networks:\\n75.       - docker-hadoop-cluster-network\\n76.     environment:\\n77.       KRB_REALM: EXAMPLE.COM\\n78.       DOMAIN_REALM: kdc.kerberos.com\\n\\n\\nFile: flink-python/setup.py\\nLine number range: 31 - 34\\nContent:\\n31. if sys.version_info < (3, 9):\\n32.     print(\"Python versions prior to 3.9 are not supported for PyFlink.\",\\n33.           file=sys.stderr)\\n34.     sys.exit(-1)\\n\\n\\nFile: flink-python/setup.py\\nLine number range: 319 - 330\\nContent:\\n319.     install_requires = [\\'py4j==0.10.9.7\\', \\'python-dateutil>=2.8.0,<3\\',\\n320.                         \\'apache-beam>=2.54.0,<=2.61.0\\',\\n321.                         \\'cloudpickle>=2.2.0\\', \\'avro>=1.12.0\\',\\n322.                         \\'pytz>=2018.3\\', \\'fastavro>=1.1.0,!=1.8.0\\', \\'requests>=2.26.0\\',\\n323.                         \\'protobuf>=3.19.0\\',\\n324.                         \\'numpy>=1.22.4\\',\\n325.                         \\'pandas>=1.3.0\\',\\n326.                         \\'pyarrow>=5.0.0,<21.0.0\\',\\n327.                         \\'pemja>=0.5.0,<0.5.4;platform_system != \"Windows\"\\',\\n328.                         \\'httplib2>=0.19.0\\',\\n329.                         \\'ruamel.yaml>=0.18.4\\',\\n330.                         apache_flink_libraries_dependency]\\n\\n\\nFile: flink-dist/src/main/resources/config.yaml\\nLine number range: 19 - 24\\nContent:\\n19. # These parameters are required for Java 17 support.\\n20. # They can be safely removed when using Java 8/11.\\n21. env:\\n22.   java:\\n23.     opts:\\n24.       all: --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED\\n\\n\\nFile: README.md\\nLine number range: 1 - 10\\nContent:\\n1. # Apache Flink\\n2. \\n3. Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.\\n4. \\n5. Learn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)\\n6. \\n7. \\n8. ### Features\\n9. \\n10. * A streaming-first runtime that supports both batch processing and data streaming programs\\n\\n\\n\\nNow generate a complete executable bash script that can successfully set up and configure the environment for this project, especially designed to run inside Docker containers. The bash script should:\\n1. Install the appropriate runtime and dependencies for the project\\'s technology stack\\n2. Install all necessary system packages and tools\\n3. Set up the project directory structure and permissions\\n4. Configure the runtime environment properly\\n5. Set up necessary environment variables and configurations\\n6. Follow bash scripting best practices for error handling and security\\n\\nMake sure the bash script is self-contained and can set up the project environment from scratch in a Docker container environment.\\n' additional_kwargs={} response_metadata={} id='4c8930f8-76aa-4b8b-a5a1-f6953e3d06ec'",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "AIMessage",
                    "content": "content='```bash\\n#!/bin/bash\\n\\n# Exit immediately if a command exits with a non-zero status\\nset -e\\n\\n# Colors for output\\nRED=\\'\\\\033[0;31m\\'\\nGREEN=\\'\\\\033[0;32m\\'\\nYELLOW=\\'\\\\033[1;33m\\'\\nNC=\\'\\\\033[0m\\' # No Color\\n\\n# Logging functions\\nlog() {\\n    echo -e \"${GREEN}[$(date +\\'%Y-%m-%d %H:%M:%S\\')] $1${NC}\"\\n}\\n\\nerror() {\\n    echo -e \"${RED}[ERROR] $1${NC}\" >&2\\n}\\n\\nwarning() {\\n    echo -e \"${YELLOW}[WARNING] $1${NC}\"\\n}\\n\\n# Check if Python 3.9+ is installed\\ncheck_python() {\\n    if command -v python3 &> /dev/null; then\\n        PYTHON_VERSION=$(python3 -c \\'import sys; print(\".\".join(map(str, sys.version_info[:2])))\\')\\n        if [[ \"$PYTHON_VERSION\" < \"3.9\" ]]; then\\n            error \"Python 3.9+ is required but found Python $PYTHON_VERSION\"\\n            exit 1\\n        fi\\n        log \"Python $PYTHON_VERSION found\"\\n    else\\n        error \"Python 3.9+ is required but not installed\"\\n        exit 1\\n    fi\\n}\\n\\n# Install system dependencies\\ninstall_system_deps() {\\n    log \"Installing system dependencies...\"\\n    apt-get update\\n    apt-get install -y openjdk-17-jdk python3-pip python3-venv gcc\\n    log \"System dependencies installed\"\\n}\\n\\n# Set up Python virtual environment\\nsetup_venv() {\\n    log \"Setting up Python virtual environment...\"\\n    python3 -m venv venv\\n    source venv/bin/activate\\n    log \"Virtual environment created and activated\"\\n}\\n\\n# Install Python dependencies\\ninstall_python_deps() {\\n    log \"Installing Python dependencies...\"\\n    pip install --upgrade pip\\n    pip install -r flink-python/requirements.txt\\n    log \"Python dependencies installed\"\\n}\\n\\n# Set environment variables\\nsetup_env() {\\n    log \"Setting up environment variables...\"\\n    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\\n    export PATH=$JAVA_HOME/bin:$PATH\\n    log \"Environment variables set\"\\n}\\n\\n# Configure Java options for Flink\\nconfigure_java_opts() {\\n    log \"Configuring Java options for Flink...\"\\n    cat <<EOF > flink-dist/src/main/resources/config.yaml\\nenv:\\n  java:\\n    opts:\\n      all: --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED\\nEOF\\n    log \"Java options configured\"\\n}\\n\\n# Main setup function\\nmain() {\\n    log \"Starting Apache Flink environment setup...\"\\n\\n    check_python\\n    install_system_deps\\n    setup_venv\\n    install_python_deps\\n    setup_env\\n    configure_java_opts\\n\\n    log \"Environment setup completed successfully!\"\\n}\\n\\n# Run main function\\nmain \"$@\"\\n```\\n\\n### Explanation:\\n1. **Error Handling and Logging**: The script uses `set -e` to exit on any error and provides logging functions for consistent output.\\n2. **Python Version Check**: Ensures Python 3.9+ is installed, as required by the project.\\n3. **System Dependencies**: Installs necessary system packages, including Java 17 and Python tools.\\n4. **Virtual Environment**: Sets up a Python virtual environment to isolate dependencies.\\n5. **Environment Variables**: Configures necessary environment variables, including `JAVA_HOME`.\\n6. **Java Options**: Configures Java options required for Flink with Java 17.\\n7. **Idempotency**: The script is designed to be idempotent, meaning it can be safely run multiple times without causing issues.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1101, 'prompt_tokens': 2549, 'total_tokens': 3650, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'finish_reason': 'stop', 'logprobs': None} id='run--64037b7f-a5a2-4d4c-ad80-c50a9b8f7162-0' usage_metadata={'input_tokens': 2549, 'output_tokens': 1101, 'total_tokens': 3650, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                }
            ],
            "env_implement_file_messages": [
                {
                    "type": "AIMessage",
                    "content": "content='' additional_kwargs={'tool_calls': [{'id': 'call_eKD9p0GgAMzvz05iOjMqI8Pk', 'function': {'arguments': '{\"relative_path\":\"prometheus_setup.sh\",\"content\":\"#!/bin/bash\\\\n\\\\n# Exit immediately if a command exits with a non-zero status\\\\nset -e\\\\n\\\\n# Colors for output\\\\nRED=\\'\\\\\\\\033[0;31m\\'\\\\nGREEN=\\'\\\\\\\\033[0;32m\\'\\\\nYELLOW=\\'\\\\\\\\033[1;33m\\'\\\\nNC=\\'\\\\\\\\033[0m\\' # No Color\\\\n\\\\n# Logging functions\\\\nlog() {\\\\n    echo -e \\\\\"${GREEN}[$(date +\\'%Y-%m-%d %H:%M:%S\\')] $1${NC}\\\\\"\\\\n}\\\\n\\\\nerror() {\\\\n    echo -e \\\\\"${RED}[ERROR] $1${NC}\\\\\" >&2\\\\n}\\\\n\\\\nwarning() {\\\\n    echo -e \\\\\"${YELLOW}[WARNING] $1${NC}\\\\\"\\\\n}\\\\n\\\\n# Check if Python 3.9+ is installed\\\\ncheck_python() {\\\\n    if command -v python3 &> /dev/null; then\\\\n        PYTHON_VERSION=$(python3 -c \\'import sys; print(\\\\\".\\\\\".join(map(str, sys.version_info[:2])))\\')\\\\n        if [[ \\\\\"$PYTHON_VERSION\\\\\" < \\\\\"3.9\\\\\" ]]; then\\\\n            error \\\\\"Python 3.9+ is required but found Python $PYTHON_VERSION\\\\\"\\\\n            exit 1\\\\n        fi\\\\n        log \\\\\"Python $PYTHON_VERSION found\\\\\"\\\\n    else\\\\n        error \\\\\"Python 3.9+ is required but not installed\\\\\"\\\\n        exit 1\\\\n    fi\\\\n}\\\\n\\\\n# Install system dependencies\\\\ninstall_system_deps() {\\\\n    log \\\\\"Installing system dependencies...\\\\\"\\\\n    apt-get update\\\\n    apt-get install -y openjdk-17-jdk python3-pip python3-venv gcc\\\\n    log \\\\\"System dependencies installed\\\\\"\\\\n}\\\\n\\\\n# Set up Python virtual environment\\\\nsetup_venv() {\\\\n    log \\\\\"Setting up Python virtual environment...\\\\\"\\\\n    python3 -m venv venv\\\\n    source venv/bin/activate\\\\n    log \\\\\"Virtual environment created and activated\\\\\"\\\\n}\\\\n\\\\n# Install Python dependencies\\\\ninstall_python_deps() {\\\\n    log \\\\\"Installing Python dependencies...\\\\\"\\\\n    pip install --upgrade pip\\\\n    pip install -r flink-python/requirements.txt\\\\n    log \\\\\"Python dependencies installed\\\\\"\\\\n}\\\\n\\\\n# Set environment variables\\\\nsetup_env() {\\\\n    log \\\\\"Setting up environment variables...\\\\\"\\\\n    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\\\\n    export PATH=$JAVA_HOME/bin:$PATH\\\\n    log \\\\\"Environment variables set\\\\\"\\\\n}\\\\n\\\\n# Configure Java options for Flink\\\\nconfigure_java_opts() {\\\\n    log \\\\\"Configuring Java options for Flink...\\\\\"\\\\n    cat <<EOF > flink-dist/src/main/resources/config.yaml\\\\nenv:\\\\n  java:\\\\n    opts:\\\\n      all: --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED\\\\nEOF\\\\n    log \\\\\"Java options configured\\\\\"\\\\n}\\\\n\\\\n# Main setup function\\\\nmain() {\\\\n    log \\\\\"Starting Apache Flink environment setup...\\\\\"\\\\n\\\\n    check_python\\\\n    install_system_deps\\\\n    setup_venv\\\\n    install_python_deps\\\\n    setup_env\\\\n    configure_java_opts\\\\n\\\\n    log \\\\\"Environment setup completed successfully!\\\\\"\\\\n}\\\\n\\\\n# Run main function\\\\nmain \\\\\"$@\\\\\"\\\\n\"}', 'name': 'create_file'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1031, 'prompt_tokens': 40589, 'total_tokens': 41620, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--d0d23788-06fe-49eb-9752-9fc09a49c4df-0' tool_calls=[{'name': 'create_file', 'args': {'relative_path': 'prometheus_setup.sh', 'content': '#!/bin/bash\\n\\n# Exit immediately if a command exits with a non-zero status\\nset -e\\n\\n# Colors for output\\nRED=\\'\\\\033[0;31m\\'\\nGREEN=\\'\\\\033[0;32m\\'\\nYELLOW=\\'\\\\033[1;33m\\'\\nNC=\\'\\\\033[0m\\' # No Color\\n\\n# Logging functions\\nlog() {\\n    echo -e \"${GREEN}[$(date +\\'%Y-%m-%d %H:%M:%S\\')] $1${NC}\"\\n}\\n\\nerror() {\\n    echo -e \"${RED}[ERROR] $1${NC}\" >&2\\n}\\n\\nwarning() {\\n    echo -e \"${YELLOW}[WARNING] $1${NC}\"\\n}\\n\\n# Check if Python 3.9+ is installed\\ncheck_python() {\\n    if command -v python3 &> /dev/null; then\\n        PYTHON_VERSION=$(python3 -c \\'import sys; print(\".\".join(map(str, sys.version_info[:2])))\\')\\n        if [[ \"$PYTHON_VERSION\" < \"3.9\" ]]; then\\n            error \"Python 3.9+ is required but found Python $PYTHON_VERSION\"\\n            exit 1\\n        fi\\n        log \"Python $PYTHON_VERSION found\"\\n    else\\n        error \"Python 3.9+ is required but not installed\"\\n        exit 1\\n    fi\\n}\\n\\n# Install system dependencies\\ninstall_system_deps() {\\n    log \"Installing system dependencies...\"\\n    apt-get update\\n    apt-get install -y openjdk-17-jdk python3-pip python3-venv gcc\\n    log \"System dependencies installed\"\\n}\\n\\n# Set up Python virtual environment\\nsetup_venv() {\\n    log \"Setting up Python virtual environment...\"\\n    python3 -m venv venv\\n    source venv/bin/activate\\n    log \"Virtual environment created and activated\"\\n}\\n\\n# Install Python dependencies\\ninstall_python_deps() {\\n    log \"Installing Python dependencies...\"\\n    pip install --upgrade pip\\n    pip install -r flink-python/requirements.txt\\n    log \"Python dependencies installed\"\\n}\\n\\n# Set environment variables\\nsetup_env() {\\n    log \"Setting up environment variables...\"\\n    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\\n    export PATH=$JAVA_HOME/bin:$PATH\\n    log \"Environment variables set\"\\n}\\n\\n# Configure Java options for Flink\\nconfigure_java_opts() {\\n    log \"Configuring Java options for Flink...\"\\n    cat <<EOF > flink-dist/src/main/resources/config.yaml\\nenv:\\n  java:\\n    opts:\\n      all: --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED\\nEOF\\n    log \"Java options configured\"\\n}\\n\\n# Main setup function\\nmain() {\\n    log \"Starting Apache Flink environment setup...\"\\n\\n    check_python\\n    install_system_deps\\n    setup_venv\\n    install_python_deps\\n    setup_env\\n    configure_java_opts\\n\\n    log \"Environment setup completed successfully!\"\\n}\\n\\n# Run main function\\nmain \"$@\"\\n'}, 'id': 'call_eKD9p0GgAMzvz05iOjMqI8Pk', 'type': 'tool_call'}] usage_metadata={'input_tokens': 40589, 'output_tokens': 1031, 'total_tokens': 41620, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "ToolMessage",
                    "content": "content='The file prometheus_setup.sh has been created.' name='create_file' id='e44e04a6-ef1a-405d-9f7d-813462d285d1' tool_call_id='call_eKD9p0GgAMzvz05iOjMqI8Pk'",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                },
                {
                    "type": "AIMessage",
                    "content": "content='The bash script has been saved in the project as `prometheus_setup.sh`.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 41638, 'total_tokens': 41655, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 41600}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'finish_reason': 'stop', 'logprobs': None} id='run--3e2dc08a-a880-4c27-920a-2ddcde3aa0f1-0' usage_metadata={'input_tokens': 41638, 'output_tokens': 17, 'total_tokens': 41655, 'input_token_details': {'audio': 0, 'cache_read': 41600}, 'output_token_details': {'audio': 0, 'reasoning': 0}}",
                    "relative_path": null,
                    "start_line_number": null,
                    "end_line_number": null
                }
            ],
            "env_implement_execute_messages": [],
            "env_implement_bash_content": "diff --git a/prometheus.Dockerfile b/prometheus.Dockerfile\nnew file mode 100644\nindex 00000000000..1616a728241\n--- /dev/null\n+++ b/prometheus.Dockerfile\n@@ -0,0 +1,22 @@\n+FROM ubuntu:24.04\n+\n+# Avoid timezone prompts during package installation\n+ENV DEBIAN_FRONTEND=noninteractive\n+ENV TZ=UTC\n+\n+# Set working directory\n+WORKDIR /app\n+\n+# Install essential build and development tools\n+RUN apt-get update && apt-get install -y     build-essential     cmake     git     curl     wget     python3     python3-pip     python3-dev     python3-venv     nodejs     npm     default-jdk     gcc     g++     gdb     postgresql-client     mysql-client     sqlite3     iputils-ping     vim     nano     zip     unzip     ca-certificates     gnupg     lsb-release\n+\n+RUN mkdir -p /etc/apt/keyrings     && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg     && echo     \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu     $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null\n+\n+RUN apt-get update && apt-get install -y docker-ce-cli\n+\n+RUN apt-get clean\n+RUN rm -rf /var/lib/apt/lists/*\n+RUN ln -s /usr/bin/python3 /usr/bin/python\n+\n+# Copy project files\n+COPY . /app/\ndiff --git a/prometheus_setup.sh b/prometheus_setup.sh\nnew file mode 100644\nindex 00000000000..a71ec121f0a\n--- /dev/null\n+++ b/prometheus_setup.sh\n@@ -0,0 +1,99 @@\n+#!/bin/bash\n+\n+# Exit immediately if a command exits with a non-zero status\n+set -e\n+\n+# Colors for output\n+RED='\\033[0;31m'\n+GREEN='\\033[0;32m'\n+YELLOW='\\033[1;33m'\n+NC='\\033[0m' # No Color\n+\n+# Logging functions\n+log() {\n+    echo -e \"${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}\"\n+}\n+\n+error() {\n+    echo -e \"${RED}[ERROR] $1${NC}\" >&2\n+}\n+\n+warning() {\n+    echo -e \"${YELLOW}[WARNING] $1${NC}\"\n+}\n+\n+# Check if Python 3.9+ is installed\n+check_python() {\n+    if command -v python3 &> /dev/null; then\n+        PYTHON_VERSION=$(python3 -c 'import sys; print(\".\".join(map(str, sys.version_info[:2])))')\n+        if [[ \"$PYTHON_VERSION\" < \"3.9\" ]]; then\n+            error \"Python 3.9+ is required but found Python $PYTHON_VERSION\"\n+            exit 1\n+        fi\n+        log \"Python $PYTHON_VERSION found\"\n+    else\n+        error \"Python 3.9+ is required but not installed\"\n+        exit 1\n+    fi\n+}\n+\n+# Install system dependencies\n+install_system_deps() {\n+    log \"Installing system dependencies...\"\n+    apt-get update\n+    apt-get install -y openjdk-17-jdk python3-pip python3-venv gcc\n+    log \"System dependencies installed\"\n+}\n+\n+# Set up Python virtual environment\n+setup_venv() {\n+    log \"Setting up Python virtual environment...\"\n+    python3 -m venv venv\n+    source venv/bin/activate\n+    log \"Virtual environment created and activated\"\n+}\n+\n+# Install Python dependencies\n+install_python_deps() {\n+    log \"Installing Python dependencies...\"\n+    pip install --upgrade pip\n+    pip install -r flink-python/requirements.txt\n+    log \"Python dependencies installed\"\n+}\n+\n+# Set environment variables\n+setup_env() {\n+    log \"Setting up environment variables...\"\n+    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n+    export PATH=$JAVA_HOME/bin:$PATH\n+    log \"Environment variables set\"\n+}\n+\n+# Configure Java options for Flink\n+configure_java_opts() {\n+    log \"Configuring Java options for Flink...\"\n+    cat <<EOF > flink-dist/src/main/resources/config.yaml\n+env:\n+  java:\n+    opts:\n+      all: --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED\n+EOF\n+    log \"Java options configured\"\n+}\n+\n+# Main setup function\n+main() {\n+    log \"Starting Apache Flink environment setup...\"\n+\n+    check_python\n+    install_system_deps\n+    setup_venv\n+    install_python_deps\n+    setup_env\n+    configure_java_opts\n+\n+    log \"Environment setup completed successfully!\"\n+}\n+\n+# Run main function\n+main \"$@\"\n",
            "env_implement_bash_path": "prometheus_setup.sh"
        },
        "timestamp": "2025-09-19T10:53:44.124516"
    }
}